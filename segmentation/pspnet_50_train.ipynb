{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras_segmentation.pretrained import pspnet_50_ADE_20K\n",
    "from keras_segmentation.models.pspnet import pspnet_50\n",
    "from keras_segmentation.models.model_utils import transfer_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OS\n",
      " Volume Serial Number is 7A94-1164\n",
      "\n",
      " Directory of C:\\apps\\segmentation\n",
      "\n",
      "11/01/2019  01:11 PM    <DIR>          .\n",
      "11/01/2019  01:11 PM    <DIR>          ..\n",
      "11/01/2019  10:28 AM    <DIR>          .ipynb_checkpoints\n",
      "09/22/2017  07:20 PM    <DIR>          dataset1\n",
      "11/01/2019  10:30 AM            20,981 Untitled.ipynb\n",
      "11/01/2019  01:11 PM           182,174 Untitled1.ipynb\n",
      "               2 File(s)        203,155 bytes\n",
      "               4 Dir(s)  160,139,116,544 bytes free\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.engine.training.Model"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\vash\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From c:\\users\\vash\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\vash\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\vash\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras_segmentation-0.3.0-py3.7.egg\\keras_segmentation\\models\\_pspnet_2.py:37: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = pspnet_50_ADE_20K()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 473, 473, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_1_3x3_s2 (Conv2D)         (None, 237, 237, 64) 1728        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_1_3x3_s2_bn (BatchNormali (None, 237, 237, 64) 256         conv1_1_3x3_s2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 237, 237, 64) 0           conv1_1_3x3_s2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_2_3x3 (Conv2D)            (None, 237, 237, 64) 36864       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1_2_3x3_bn (BatchNormalizat (None, 237, 237, 64) 256         conv1_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 237, 237, 64) 0           conv1_2_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1_3_3x3 (Conv2D)            (None, 237, 237, 128 73728       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1_3_3x3_bn (BatchNormalizat (None, 237, 237, 128 512         conv1_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 237, 237, 128 0           conv1_3_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 119, 119, 128 0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 119, 119, 128 0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_reduce (Conv2D)     (None, 119, 119, 64) 8192        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_reduce_bn (BatchNor (None, 119, 119, 64) 256         conv2_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 119, 119, 64) 0           conv2_1_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 121, 121, 64) 0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_3x3 (Conv2D)            (None, 119, 119, 64) 36864       zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_3x3_bn (BatchNormalizat (None, 119, 119, 64) 256         conv2_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 119, 119, 64) 0           conv2_1_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_increase (Conv2D)   (None, 119, 119, 256 16384       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_proj (Conv2D)       (None, 119, 119, 256 32768       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_increase_bn (BatchN (None, 119, 119, 256 1024        conv2_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_proj_bn (BatchNorma (None, 119, 119, 256 1024        conv2_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 119, 119, 256 0           conv2_1_1x1_increase_bn[0][0]    \n",
      "                                                                 conv2_1_1x1_proj_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 119, 119, 256 0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_reduce (Conv2D)     (None, 119, 119, 64) 16384       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_reduce_bn (BatchNor (None, 119, 119, 64) 256         conv2_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 119, 119, 64) 0           conv2_2_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 121, 121, 64) 0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_3x3 (Conv2D)            (None, 119, 119, 64) 36864       zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_3x3_bn (BatchNormalizat (None, 119, 119, 64) 256         conv2_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 119, 119, 64) 0           conv2_2_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_increase (Conv2D)   (None, 119, 119, 256 16384       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_increase_bn (BatchN (None, 119, 119, 256 1024        conv2_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 119, 119, 256 0           conv2_2_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 119, 119, 256 0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_reduce (Conv2D)     (None, 119, 119, 64) 16384       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_reduce_bn (BatchNor (None, 119, 119, 64) 256         conv2_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 119, 119, 64) 0           conv2_3_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 121, 121, 64) 0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_3x3 (Conv2D)            (None, 119, 119, 64) 36864       zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_3x3_bn (BatchNormalizat (None, 119, 119, 64) 256         conv2_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 119, 119, 64) 0           conv2_3_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_increase (Conv2D)   (None, 119, 119, 256 16384       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_increase_bn (BatchN (None, 119, 119, 256 1024        conv2_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 119, 119, 256 0           conv2_3_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 119, 119, 256 0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_reduce (Conv2D)     (None, 60, 60, 128)  32768       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_reduce_bn (BatchNor (None, 60, 60, 128)  512         conv3_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 60, 60, 128)  0           conv3_1_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 62, 62, 128)  0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_3x3 (Conv2D)            (None, 60, 60, 128)  147456      zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_3x3_bn (BatchNormalizat (None, 60, 60, 128)  512         conv3_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 60, 60, 128)  0           conv3_1_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_increase (Conv2D)   (None, 60, 60, 512)  65536       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_proj (Conv2D)       (None, 60, 60, 512)  131072      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_increase_bn (BatchN (None, 60, 60, 512)  2048        conv3_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_proj_bn (BatchNorma (None, 60, 60, 512)  2048        conv3_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 60, 60, 512)  0           conv3_1_1x1_increase_bn[0][0]    \n",
      "                                                                 conv3_1_1x1_proj_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 60, 60, 512)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_reduce (Conv2D)     (None, 60, 60, 128)  65536       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_reduce_bn (BatchNor (None, 60, 60, 128)  512         conv3_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 60, 60, 128)  0           conv3_2_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 62, 62, 128)  0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_3x3 (Conv2D)            (None, 60, 60, 128)  147456      zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_3x3_bn (BatchNormalizat (None, 60, 60, 128)  512         conv3_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 60, 60, 128)  0           conv3_2_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_increase (Conv2D)   (None, 60, 60, 512)  65536       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_increase_bn (BatchN (None, 60, 60, 512)  2048        conv3_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 60, 60, 512)  0           conv3_2_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 60, 60, 512)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_reduce (Conv2D)     (None, 60, 60, 128)  65536       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_reduce_bn (BatchNor (None, 60, 60, 128)  512         conv3_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 60, 60, 128)  0           conv3_3_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPadding2D (None, 62, 62, 128)  0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_3x3 (Conv2D)            (None, 60, 60, 128)  147456      zero_padding2d_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_3x3_bn (BatchNormalizat (None, 60, 60, 128)  512         conv3_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 60, 60, 128)  0           conv3_3_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv3_3_1x1_increase (Conv2D)   (None, 60, 60, 512)  65536       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_increase_bn (BatchN (None, 60, 60, 512)  2048        conv3_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 60, 60, 512)  0           conv3_3_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 60, 60, 512)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_reduce (Conv2D)     (None, 60, 60, 128)  65536       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_reduce_bn (BatchNor (None, 60, 60, 128)  512         conv3_4_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 60, 60, 128)  0           conv3_4_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPadding2D (None, 62, 62, 128)  0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_3x3 (Conv2D)            (None, 60, 60, 128)  147456      zero_padding2d_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_3x3_bn (BatchNormalizat (None, 60, 60, 128)  512         conv3_4_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 60, 60, 128)  0           conv3_4_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_increase (Conv2D)   (None, 60, 60, 512)  65536       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_increase_bn (BatchN (None, 60, 60, 512)  2048        conv3_4_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 60, 60, 512)  0           conv3_4_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 60, 60, 512)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_reduce (Conv2D)     (None, 60, 60, 256)  131072      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 60, 60, 256)  0           conv4_1_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPadding2D (None, 64, 64, 256)  0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 60, 60, 256)  0           conv4_1_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_proj (Conv2D)       (None, 60, 60, 1024) 524288      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_proj_bn (BatchNorma (None, 60, 60, 1024) 4096        conv4_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 60, 60, 1024) 0           conv4_1_1x1_increase_bn[0][0]    \n",
      "                                                                 conv4_1_1x1_proj_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 60, 60, 1024) 0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_reduce (Conv2D)     (None, 60, 60, 256)  262144      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 60, 60, 256)  0           conv4_2_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPadding2D (None, 64, 64, 256)  0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_9[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 60, 60, 256)  0           conv4_2_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 60, 60, 1024) 0           conv4_2_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 60, 60, 1024) 0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_reduce (Conv2D)     (None, 60, 60, 256)  262144      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 60, 60, 256)  0           conv4_3_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPadding2 (None, 64, 64, 256)  0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 60, 60, 256)  0           conv4_3_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 60, 60, 1024) 0           conv4_3_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 60, 60, 1024) 0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_reduce (Conv2D)     (None, 60, 60, 256)  262144      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_4_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 60, 60, 256)  0           conv4_4_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPadding2 (None, 64, 64, 256)  0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_11[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_4_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 60, 60, 256)  0           conv4_4_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_4_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 60, 60, 1024) 0           conv4_4_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 60, 60, 1024) 0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_reduce (Conv2D)     (None, 60, 60, 256)  262144      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_5_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 60, 60, 256)  0           conv4_5_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPadding2 (None, 64, 64, 256)  0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_12[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_5_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 60, 60, 256)  0           conv4_5_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_5_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 60, 60, 1024) 0           conv4_5_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 60, 60, 1024) 0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_reduce (Conv2D)     (None, 60, 60, 256)  262144      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_6_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 60, 60, 256)  0           conv4_6_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPadding2 (None, 64, 64, 256)  0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_13[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_6_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 60, 60, 256)  0           conv4_6_3x3_bn[0][0]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_6_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 60, 60, 1024) 0           conv4_6_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 60, 60, 1024) 0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_reduce (Conv2D)     (None, 60, 60, 512)  524288      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_reduce_bn (BatchNor (None, 60, 60, 512)  2048        conv5_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 60, 60, 512)  0           conv5_1_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_14 (ZeroPadding2 (None, 68, 68, 512)  0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_3x3 (Conv2D)            (None, 60, 60, 512)  2359296     zero_padding2d_14[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_3x3_bn (BatchNormalizat (None, 60, 60, 512)  2048        conv5_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 60, 60, 512)  0           conv5_1_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_increase (Conv2D)   (None, 60, 60, 2048) 1048576     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_proj (Conv2D)       (None, 60, 60, 2048) 2097152     activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_increase_bn (BatchN (None, 60, 60, 2048) 8192        conv5_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_proj_bn (BatchNorma (None, 60, 60, 2048) 8192        conv5_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 60, 60, 2048) 0           conv5_1_1x1_increase_bn[0][0]    \n",
      "                                                                 conv5_1_1x1_proj_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 60, 60, 2048) 0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_reduce (Conv2D)     (None, 60, 60, 512)  1048576     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_reduce_bn (BatchNor (None, 60, 60, 512)  2048        conv5_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 60, 60, 512)  0           conv5_2_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_15 (ZeroPadding2 (None, 68, 68, 512)  0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_3x3 (Conv2D)            (None, 60, 60, 512)  2359296     zero_padding2d_15[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_3x3_bn (BatchNormalizat (None, 60, 60, 512)  2048        conv5_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 60, 60, 512)  0           conv5_2_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_increase (Conv2D)   (None, 60, 60, 2048) 1048576     activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_increase_bn (BatchN (None, 60, 60, 2048) 8192        conv5_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 60, 60, 2048) 0           conv5_2_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 60, 60, 2048) 0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_reduce (Conv2D)     (None, 60, 60, 512)  1048576     activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_reduce_bn (BatchNor (None, 60, 60, 512)  2048        conv5_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 60, 60, 512)  0           conv5_3_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_16 (ZeroPadding2 (None, 68, 68, 512)  0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_3x3 (Conv2D)            (None, 60, 60, 512)  2359296     zero_padding2d_16[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_3x3_bn (BatchNormalizat (None, 60, 60, 512)  2048        conv5_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 60, 60, 512)  0           conv5_3_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_increase (Conv2D)   (None, 60, 60, 2048) 1048576     activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_increase_bn (BatchN (None, 60, 60, 2048) 8192        conv5_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 60, 60, 2048) 0           conv5_3_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 60, 60, 2048) 0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 6, 6, 2048)   0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 3, 3, 2048)   0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 2, 2, 2048)   0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 2048)   0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool6_conv (Conv2D)     (None, 6, 6, 512)    1048576     average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool3_conv (Conv2D)     (None, 3, 3, 512)    1048576     average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool2_conv (Conv2D)     (None, 2, 2, 512)    1048576     average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool1_conv (Conv2D)     (None, 1, 1, 512)    1048576     average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool6_conv_bn (BatchNor (None, 6, 6, 512)    2048        conv5_3_pool6_conv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool3_conv_bn (BatchNor (None, 3, 3, 512)    2048        conv5_3_pool3_conv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool2_conv_bn (BatchNor (None, 2, 2, 512)    2048        conv5_3_pool2_conv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool1_conv_bn (BatchNor (None, 1, 1, 512)    2048        conv5_3_pool1_conv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 6, 6, 512)    0           conv5_3_pool6_conv_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 3, 3, 512)    0           conv5_3_pool3_conv_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 2, 2, 512)    0           conv5_3_pool2_conv_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 1, 1, 512)    0           conv5_3_pool1_conv_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "interp_4 (Interp)               (None, 60, 60, 512)  0           activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "interp_3 (Interp)               (None, 60, 60, 512)  0           activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "interp_2 (Interp)               (None, 60, 60, 512)  0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "interp_1 (Interp)               (None, 60, 60, 512)  0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 60, 60, 4096) 0           activation_52[0][0]              \n",
      "                                                                 interp_4[0][0]                   \n",
      "                                                                 interp_3[0][0]                   \n",
      "                                                                 interp_2[0][0]                   \n",
      "                                                                 interp_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv5_4 (Conv2D)                (None, 60, 60, 512)  18874368    concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_4_bn (BatchNormalization) (None, 60, 60, 512)  2048        conv5_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 60, 60, 512)  0           conv5_4_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 60, 60, 512)  0           activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv6 (Conv2D)                  (None, 60, 60, 150)  76950       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "interp_5 (Interp)               (None, 473, 473, 150 0           conv6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 223729, 150)  0           interp_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 223729, 150)  0           reshape_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 46,841,174\n",
      "Trainable params: 0\n",
      "Non-trainable params: 46,841,174\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pretrained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = pspnet_50( n_classes=51 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 473, 473, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_1_3x3_s2 (Conv2D)         (None, 237, 237, 64) 1728        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_1_3x3_s2_bn (BatchNormali (None, 237, 237, 64) 256         conv1_1_3x3_s2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 237, 237, 64) 0           conv1_1_3x3_s2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_2_3x3 (Conv2D)            (None, 237, 237, 64) 36864       activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1_2_3x3_bn (BatchNormalizat (None, 237, 237, 64) 256         conv1_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 237, 237, 64) 0           conv1_2_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1_3_3x3 (Conv2D)            (None, 237, 237, 128 73728       activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1_3_3x3_bn (BatchNormalizat (None, 237, 237, 128 512         conv1_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 237, 237, 128 0           conv1_3_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 119, 119, 128 0           activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 119, 119, 128 0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_reduce (Conv2D)     (None, 119, 119, 64) 8192        activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_reduce_bn (BatchNor (None, 119, 119, 64) 256         conv2_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 119, 119, 64) 0           conv2_1_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_33 (ZeroPadding2 (None, 121, 121, 64) 0           activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_3x3 (Conv2D)            (None, 119, 119, 64) 36864       zero_padding2d_33[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_3x3_bn (BatchNormalizat (None, 119, 119, 64) 256         conv2_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 119, 119, 64) 0           conv2_1_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_increase (Conv2D)   (None, 119, 119, 256 16384       activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_proj (Conv2D)       (None, 119, 119, 256 32768       activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_increase_bn (BatchN (None, 119, 119, 256 1024        conv2_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_proj_bn (BatchNorma (None, 119, 119, 256 1024        conv2_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 119, 119, 256 0           conv2_1_1x1_increase_bn[0][0]    \n",
      "                                                                 conv2_1_1x1_proj_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 119, 119, 256 0           add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_reduce (Conv2D)     (None, 119, 119, 64) 16384       activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_reduce_bn (BatchNor (None, 119, 119, 64) 256         conv2_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 119, 119, 64) 0           conv2_2_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_34 (ZeroPadding2 (None, 121, 121, 64) 0           activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_3x3 (Conv2D)            (None, 119, 119, 64) 36864       zero_padding2d_34[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_3x3_bn (BatchNormalizat (None, 119, 119, 64) 256         conv2_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 119, 119, 64) 0           conv2_2_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_increase (Conv2D)   (None, 119, 119, 256 16384       activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_increase_bn (BatchN (None, 119, 119, 256 1024        conv2_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 119, 119, 256 0           conv2_2_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 119, 119, 256 0           add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_reduce (Conv2D)     (None, 119, 119, 64) 16384       activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_reduce_bn (BatchNor (None, 119, 119, 64) 256         conv2_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 119, 119, 64) 0           conv2_3_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_35 (ZeroPadding2 (None, 121, 121, 64) 0           activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_3x3 (Conv2D)            (None, 119, 119, 64) 36864       zero_padding2d_35[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_3x3_bn (BatchNormalizat (None, 119, 119, 64) 256         conv2_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 119, 119, 64) 0           conv2_3_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_increase (Conv2D)   (None, 119, 119, 256 16384       activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_increase_bn (BatchN (None, 119, 119, 256 1024        conv2_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 119, 119, 256 0           conv2_3_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 119, 119, 256 0           add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_reduce (Conv2D)     (None, 60, 60, 128)  32768       activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_reduce_bn (BatchNor (None, 60, 60, 128)  512         conv3_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 60, 60, 128)  0           conv3_1_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_36 (ZeroPadding2 (None, 62, 62, 128)  0           activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_3x3 (Conv2D)            (None, 60, 60, 128)  147456      zero_padding2d_36[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_3x3_bn (BatchNormalizat (None, 60, 60, 128)  512         conv3_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 60, 60, 128)  0           conv3_1_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_increase (Conv2D)   (None, 60, 60, 512)  65536       activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_proj (Conv2D)       (None, 60, 60, 512)  131072      activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_increase_bn (BatchN (None, 60, 60, 512)  2048        conv3_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_proj_bn (BatchNorma (None, 60, 60, 512)  2048        conv3_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 60, 60, 512)  0           conv3_1_1x1_increase_bn[0][0]    \n",
      "                                                                 conv3_1_1x1_proj_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 60, 60, 512)  0           add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_reduce (Conv2D)     (None, 60, 60, 128)  65536       activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_reduce_bn (BatchNor (None, 60, 60, 128)  512         conv3_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 60, 60, 128)  0           conv3_2_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_37 (ZeroPadding2 (None, 62, 62, 128)  0           activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_3x3 (Conv2D)            (None, 60, 60, 128)  147456      zero_padding2d_37[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_3x3_bn (BatchNormalizat (None, 60, 60, 128)  512         conv3_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 60, 60, 128)  0           conv3_2_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_increase (Conv2D)   (None, 60, 60, 512)  65536       activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_increase_bn (BatchN (None, 60, 60, 512)  2048        conv3_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 60, 60, 512)  0           conv3_2_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 60, 60, 512)  0           add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_reduce (Conv2D)     (None, 60, 60, 128)  65536       activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_reduce_bn (BatchNor (None, 60, 60, 128)  512         conv3_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 60, 60, 128)  0           conv3_3_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_38 (ZeroPadding2 (None, 62, 62, 128)  0           activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_3x3 (Conv2D)            (None, 60, 60, 128)  147456      zero_padding2d_38[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_3x3_bn (BatchNormalizat (None, 60, 60, 128)  512         conv3_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 60, 60, 128)  0           conv3_3_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv3_3_1x1_increase (Conv2D)   (None, 60, 60, 512)  65536       activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_increase_bn (BatchN (None, 60, 60, 512)  2048        conv3_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 60, 60, 512)  0           conv3_3_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 60, 60, 512)  0           add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_reduce (Conv2D)     (None, 60, 60, 128)  65536       activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_reduce_bn (BatchNor (None, 60, 60, 128)  512         conv3_4_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 60, 60, 128)  0           conv3_4_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_39 (ZeroPadding2 (None, 62, 62, 128)  0           activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_3x3 (Conv2D)            (None, 60, 60, 128)  147456      zero_padding2d_39[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_3x3_bn (BatchNormalizat (None, 60, 60, 128)  512         conv3_4_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 60, 60, 128)  0           conv3_4_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_increase (Conv2D)   (None, 60, 60, 512)  65536       activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_increase_bn (BatchN (None, 60, 60, 512)  2048        conv3_4_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 60, 60, 512)  0           conv3_4_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 60, 60, 512)  0           add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_reduce (Conv2D)     (None, 60, 60, 256)  131072      activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 60, 60, 256)  0           conv4_1_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_40 (ZeroPadding2 (None, 64, 64, 256)  0           activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_40[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 60, 60, 256)  0           conv4_1_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_proj (Conv2D)       (None, 60, 60, 1024) 524288      activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_proj_bn (BatchNorma (None, 60, 60, 1024) 4096        conv4_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 60, 60, 1024) 0           conv4_1_1x1_increase_bn[0][0]    \n",
      "                                                                 conv4_1_1x1_proj_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 60, 60, 1024) 0           add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_reduce (Conv2D)     (None, 60, 60, 256)  262144      activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 60, 60, 256)  0           conv4_2_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_41 (ZeroPadding2 (None, 64, 64, 256)  0           activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_41[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 60, 60, 256)  0           conv4_2_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 60, 60, 1024) 0           conv4_2_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 60, 60, 1024) 0           add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_reduce (Conv2D)     (None, 60, 60, 256)  262144      activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 60, 60, 256)  0           conv4_3_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_42 (ZeroPadding2 (None, 64, 64, 256)  0           activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_42[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 60, 60, 256)  0           conv4_3_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 60, 60, 1024) 0           conv4_3_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 60, 60, 1024) 0           add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_reduce (Conv2D)     (None, 60, 60, 256)  262144      activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_4_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 60, 60, 256)  0           conv4_4_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_43 (ZeroPadding2 (None, 64, 64, 256)  0           activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_43[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_4_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 60, 60, 256)  0           conv4_4_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_4_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 60, 60, 1024) 0           conv4_4_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 60, 60, 1024) 0           add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_reduce (Conv2D)     (None, 60, 60, 256)  262144      activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_5_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 60, 60, 256)  0           conv4_5_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_44 (ZeroPadding2 (None, 64, 64, 256)  0           activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_44[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_5_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 60, 60, 256)  0           conv4_5_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_5_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 60, 60, 1024) 0           conv4_5_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 60, 60, 1024) 0           add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_reduce (Conv2D)     (None, 60, 60, 256)  262144      activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_6_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 60, 60, 256)  0           conv4_6_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_45 (ZeroPadding2 (None, 64, 64, 256)  0           activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_45[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_6_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 60, 60, 256)  0           conv4_6_3x3_bn[0][0]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_6_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 60, 60, 1024) 0           conv4_6_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 60, 60, 1024) 0           add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_reduce (Conv2D)     (None, 60, 60, 512)  524288      activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_reduce_bn (BatchNor (None, 60, 60, 512)  2048        conv5_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 60, 60, 512)  0           conv5_1_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_46 (ZeroPadding2 (None, 68, 68, 512)  0           activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_3x3 (Conv2D)            (None, 60, 60, 512)  2359296     zero_padding2d_46[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_3x3_bn (BatchNormalizat (None, 60, 60, 512)  2048        conv5_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 60, 60, 512)  0           conv5_1_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_increase (Conv2D)   (None, 60, 60, 2048) 1048576     activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_proj (Conv2D)       (None, 60, 60, 2048) 2097152     activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_increase_bn (BatchN (None, 60, 60, 2048) 8192        conv5_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_proj_bn (BatchNorma (None, 60, 60, 2048) 8192        conv5_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 60, 60, 2048) 0           conv5_1_1x1_increase_bn[0][0]    \n",
      "                                                                 conv5_1_1x1_proj_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 60, 60, 2048) 0           add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_reduce (Conv2D)     (None, 60, 60, 512)  1048576     activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_reduce_bn (BatchNor (None, 60, 60, 512)  2048        conv5_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 60, 60, 512)  0           conv5_2_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_47 (ZeroPadding2 (None, 68, 68, 512)  0           activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_3x3 (Conv2D)            (None, 60, 60, 512)  2359296     zero_padding2d_47[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_3x3_bn (BatchNormalizat (None, 60, 60, 512)  2048        conv5_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 60, 60, 512)  0           conv5_2_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_increase (Conv2D)   (None, 60, 60, 2048) 1048576     activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_increase_bn (BatchN (None, 60, 60, 2048) 8192        conv5_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 60, 60, 2048) 0           conv5_2_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 60, 60, 2048) 0           add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_reduce (Conv2D)     (None, 60, 60, 512)  1048576     activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_reduce_bn (BatchNor (None, 60, 60, 512)  2048        conv5_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 60, 60, 512)  0           conv5_3_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_48 (ZeroPadding2 (None, 68, 68, 512)  0           activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_3x3 (Conv2D)            (None, 60, 60, 512)  2359296     zero_padding2d_48[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_3x3_bn (BatchNormalizat (None, 60, 60, 512)  2048        conv5_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 60, 60, 512)  0           conv5_3_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_increase (Conv2D)   (None, 60, 60, 2048) 1048576     activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_increase_bn (BatchN (None, 60, 60, 2048) 8192        conv5_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 60, 60, 2048) 0           conv5_3_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 60, 60, 2048) 0           add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_12 (AveragePo (None, 6, 6, 2048)   0           activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_11 (AveragePo (None, 3, 3, 2048)   0           activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_10 (AveragePo (None, 2, 2, 2048)   0           activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 1, 1, 2048)   0           activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool6_conv (Conv2D)     (None, 6, 6, 512)    1048576     average_pooling2d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool3_conv (Conv2D)     (None, 3, 3, 512)    1048576     average_pooling2d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool2_conv (Conv2D)     (None, 2, 2, 512)    1048576     average_pooling2d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool1_conv (Conv2D)     (None, 1, 1, 512)    1048576     average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool6_conv_bn (BatchNor (None, 6, 6, 512)    2048        conv5_3_pool6_conv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool3_conv_bn (BatchNor (None, 3, 3, 512)    2048        conv5_3_pool3_conv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool2_conv_bn (BatchNor (None, 2, 2, 512)    2048        conv5_3_pool2_conv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool1_conv_bn (BatchNor (None, 1, 1, 512)    2048        conv5_3_pool1_conv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 6, 6, 512)    0           conv5_3_pool6_conv_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 3, 3, 512)    0           conv5_3_pool3_conv_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 2, 2, 512)    0           conv5_3_pool2_conv_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 1, 1, 512)    0           conv5_3_pool1_conv_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "interp_14 (Interp)              (None, 60, 60, 512)  0           activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "interp_13 (Interp)              (None, 60, 60, 512)  0           activation_171[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "interp_12 (Interp)              (None, 60, 60, 512)  0           activation_170[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "interp_11 (Interp)              (None, 60, 60, 512)  0           activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 60, 60, 4096) 0           activation_168[0][0]             \n",
      "                                                                 interp_14[0][0]                  \n",
      "                                                                 interp_13[0][0]                  \n",
      "                                                                 interp_12[0][0]                  \n",
      "                                                                 interp_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv5_4 (Conv2D)                (None, 60, 60, 512)  18874368    concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_4_bn (BatchNormalization) (None, 60, 60, 512)  2048        conv5_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 60, 60, 512)  0           conv5_4_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 60, 60, 512)  0           activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv6 (Conv2D)                  (None, 60, 60, 51)   26163       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "interp_15 (Interp)              (None, 473, 473, 51) 0           conv6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 223729, 51)   0           interp_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 223729, 51)   0           reshape_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 46,790,387\n",
      "Trainable params: 46,731,763\n",
      "Non-trainable params: 58,624\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying weights \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "225it [01:19,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied weights of 120 layers and skipped 1 layers\n"
     ]
    }
   ],
   "source": [
    "transfer_weights( pretrained_model , new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in new_model.layers:\n",
    "    i.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.convolutional.Conv2D at 0x26fe52b6ac8>,\n",
       " <keras_segmentation.models._pspnet_2.Interp at 0x26fe8493e08>,\n",
       " <keras.layers.core.Reshape at 0x26fe84b3a48>,\n",
       " <keras.layers.core.Activation at 0x26fe84b3808>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.layers[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in new_model.layers[-4:]:\n",
    "    i.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 473, 473, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_1_3x3_s2 (Conv2D)         (None, 237, 237, 64) 1728        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_1_3x3_s2_bn (BatchNormali (None, 237, 237, 64) 256         conv1_1_3x3_s2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 237, 237, 64) 0           conv1_1_3x3_s2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_2_3x3 (Conv2D)            (None, 237, 237, 64) 36864       activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1_2_3x3_bn (BatchNormalizat (None, 237, 237, 64) 256         conv1_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 237, 237, 64) 0           conv1_2_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1_3_3x3 (Conv2D)            (None, 237, 237, 128 73728       activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1_3_3x3_bn (BatchNormalizat (None, 237, 237, 128 512         conv1_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 237, 237, 128 0           conv1_3_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 119, 119, 128 0           activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 119, 119, 128 0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_reduce (Conv2D)     (None, 119, 119, 64) 8192        activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_reduce_bn (BatchNor (None, 119, 119, 64) 256         conv2_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 119, 119, 64) 0           conv2_1_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_33 (ZeroPadding2 (None, 121, 121, 64) 0           activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_3x3 (Conv2D)            (None, 119, 119, 64) 36864       zero_padding2d_33[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_3x3_bn (BatchNormalizat (None, 119, 119, 64) 256         conv2_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 119, 119, 64) 0           conv2_1_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_increase (Conv2D)   (None, 119, 119, 256 16384       activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_proj (Conv2D)       (None, 119, 119, 256 32768       activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_increase_bn (BatchN (None, 119, 119, 256 1024        conv2_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_proj_bn (BatchNorma (None, 119, 119, 256 1024        conv2_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 119, 119, 256 0           conv2_1_1x1_increase_bn[0][0]    \n",
      "                                                                 conv2_1_1x1_proj_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 119, 119, 256 0           add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_reduce (Conv2D)     (None, 119, 119, 64) 16384       activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_reduce_bn (BatchNor (None, 119, 119, 64) 256         conv2_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 119, 119, 64) 0           conv2_2_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_34 (ZeroPadding2 (None, 121, 121, 64) 0           activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_3x3 (Conv2D)            (None, 119, 119, 64) 36864       zero_padding2d_34[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_3x3_bn (BatchNormalizat (None, 119, 119, 64) 256         conv2_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 119, 119, 64) 0           conv2_2_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_increase (Conv2D)   (None, 119, 119, 256 16384       activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_increase_bn (BatchN (None, 119, 119, 256 1024        conv2_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 119, 119, 256 0           conv2_2_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 119, 119, 256 0           add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_reduce (Conv2D)     (None, 119, 119, 64) 16384       activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_reduce_bn (BatchNor (None, 119, 119, 64) 256         conv2_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 119, 119, 64) 0           conv2_3_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_35 (ZeroPadding2 (None, 121, 121, 64) 0           activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_3x3 (Conv2D)            (None, 119, 119, 64) 36864       zero_padding2d_35[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_3x3_bn (BatchNormalizat (None, 119, 119, 64) 256         conv2_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 119, 119, 64) 0           conv2_3_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_increase (Conv2D)   (None, 119, 119, 256 16384       activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_increase_bn (BatchN (None, 119, 119, 256 1024        conv2_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 119, 119, 256 0           conv2_3_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 119, 119, 256 0           add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_reduce (Conv2D)     (None, 60, 60, 128)  32768       activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_reduce_bn (BatchNor (None, 60, 60, 128)  512         conv3_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 60, 60, 128)  0           conv3_1_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_36 (ZeroPadding2 (None, 62, 62, 128)  0           activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_3x3 (Conv2D)            (None, 60, 60, 128)  147456      zero_padding2d_36[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_3x3_bn (BatchNormalizat (None, 60, 60, 128)  512         conv3_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 60, 60, 128)  0           conv3_1_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_increase (Conv2D)   (None, 60, 60, 512)  65536       activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_proj (Conv2D)       (None, 60, 60, 512)  131072      activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_increase_bn (BatchN (None, 60, 60, 512)  2048        conv3_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_proj_bn (BatchNorma (None, 60, 60, 512)  2048        conv3_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 60, 60, 512)  0           conv3_1_1x1_increase_bn[0][0]    \n",
      "                                                                 conv3_1_1x1_proj_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 60, 60, 512)  0           add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_reduce (Conv2D)     (None, 60, 60, 128)  65536       activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_reduce_bn (BatchNor (None, 60, 60, 128)  512         conv3_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 60, 60, 128)  0           conv3_2_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_37 (ZeroPadding2 (None, 62, 62, 128)  0           activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_3x3 (Conv2D)            (None, 60, 60, 128)  147456      zero_padding2d_37[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_3x3_bn (BatchNormalizat (None, 60, 60, 128)  512         conv3_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 60, 60, 128)  0           conv3_2_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_increase (Conv2D)   (None, 60, 60, 512)  65536       activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_increase_bn (BatchN (None, 60, 60, 512)  2048        conv3_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 60, 60, 512)  0           conv3_2_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 60, 60, 512)  0           add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_reduce (Conv2D)     (None, 60, 60, 128)  65536       activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_reduce_bn (BatchNor (None, 60, 60, 128)  512         conv3_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 60, 60, 128)  0           conv3_3_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_38 (ZeroPadding2 (None, 62, 62, 128)  0           activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_3x3 (Conv2D)            (None, 60, 60, 128)  147456      zero_padding2d_38[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_3x3_bn (BatchNormalizat (None, 60, 60, 128)  512         conv3_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 60, 60, 128)  0           conv3_3_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv3_3_1x1_increase (Conv2D)   (None, 60, 60, 512)  65536       activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_increase_bn (BatchN (None, 60, 60, 512)  2048        conv3_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 60, 60, 512)  0           conv3_3_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 60, 60, 512)  0           add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_reduce (Conv2D)     (None, 60, 60, 128)  65536       activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_reduce_bn (BatchNor (None, 60, 60, 128)  512         conv3_4_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 60, 60, 128)  0           conv3_4_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_39 (ZeroPadding2 (None, 62, 62, 128)  0           activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_3x3 (Conv2D)            (None, 60, 60, 128)  147456      zero_padding2d_39[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_3x3_bn (BatchNormalizat (None, 60, 60, 128)  512         conv3_4_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 60, 60, 128)  0           conv3_4_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_increase (Conv2D)   (None, 60, 60, 512)  65536       activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_increase_bn (BatchN (None, 60, 60, 512)  2048        conv3_4_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 60, 60, 512)  0           conv3_4_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 60, 60, 512)  0           add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_reduce (Conv2D)     (None, 60, 60, 256)  131072      activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 60, 60, 256)  0           conv4_1_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_40 (ZeroPadding2 (None, 64, 64, 256)  0           activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_40[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 60, 60, 256)  0           conv4_1_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_proj (Conv2D)       (None, 60, 60, 1024) 524288      activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_proj_bn (BatchNorma (None, 60, 60, 1024) 4096        conv4_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 60, 60, 1024) 0           conv4_1_1x1_increase_bn[0][0]    \n",
      "                                                                 conv4_1_1x1_proj_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 60, 60, 1024) 0           add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_reduce (Conv2D)     (None, 60, 60, 256)  262144      activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 60, 60, 256)  0           conv4_2_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_41 (ZeroPadding2 (None, 64, 64, 256)  0           activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_41[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 60, 60, 256)  0           conv4_2_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 60, 60, 1024) 0           conv4_2_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 60, 60, 1024) 0           add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_reduce (Conv2D)     (None, 60, 60, 256)  262144      activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 60, 60, 256)  0           conv4_3_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_42 (ZeroPadding2 (None, 64, 64, 256)  0           activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_42[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 60, 60, 256)  0           conv4_3_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 60, 60, 1024) 0           conv4_3_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_147[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 60, 60, 1024) 0           add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_reduce (Conv2D)     (None, 60, 60, 256)  262144      activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_4_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 60, 60, 256)  0           conv4_4_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_43 (ZeroPadding2 (None, 64, 64, 256)  0           activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_43[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_4_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 60, 60, 256)  0           conv4_4_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_152[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_4_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 60, 60, 1024) 0           conv4_4_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 60, 60, 1024) 0           add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_reduce (Conv2D)     (None, 60, 60, 256)  262144      activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_5_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 60, 60, 256)  0           conv4_5_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_44 (ZeroPadding2 (None, 64, 64, 256)  0           activation_154[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_44[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_5_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 60, 60, 256)  0           conv4_5_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_5_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 60, 60, 1024) 0           conv4_5_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 60, 60, 1024) 0           add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_reduce (Conv2D)     (None, 60, 60, 256)  262144      activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_6_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 60, 60, 256)  0           conv4_6_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_45 (ZeroPadding2 (None, 64, 64, 256)  0           activation_157[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_45[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_6_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 60, 60, 256)  0           conv4_6_3x3_bn[0][0]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_6_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 60, 60, 1024) 0           conv4_6_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 60, 60, 1024) 0           add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_reduce (Conv2D)     (None, 60, 60, 512)  524288      activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_reduce_bn (BatchNor (None, 60, 60, 512)  2048        conv5_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 60, 60, 512)  0           conv5_1_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_46 (ZeroPadding2 (None, 68, 68, 512)  0           activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_3x3 (Conv2D)            (None, 60, 60, 512)  2359296     zero_padding2d_46[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_3x3_bn (BatchNormalizat (None, 60, 60, 512)  2048        conv5_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 60, 60, 512)  0           conv5_1_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_increase (Conv2D)   (None, 60, 60, 2048) 1048576     activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_proj (Conv2D)       (None, 60, 60, 2048) 2097152     activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_increase_bn (BatchN (None, 60, 60, 2048) 8192        conv5_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_proj_bn (BatchNorma (None, 60, 60, 2048) 8192        conv5_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 60, 60, 2048) 0           conv5_1_1x1_increase_bn[0][0]    \n",
      "                                                                 conv5_1_1x1_proj_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 60, 60, 2048) 0           add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_reduce (Conv2D)     (None, 60, 60, 512)  1048576     activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_reduce_bn (BatchNor (None, 60, 60, 512)  2048        conv5_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 60, 60, 512)  0           conv5_2_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_47 (ZeroPadding2 (None, 68, 68, 512)  0           activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_3x3 (Conv2D)            (None, 60, 60, 512)  2359296     zero_padding2d_47[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_3x3_bn (BatchNormalizat (None, 60, 60, 512)  2048        conv5_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 60, 60, 512)  0           conv5_2_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_increase (Conv2D)   (None, 60, 60, 2048) 1048576     activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_increase_bn (BatchN (None, 60, 60, 2048) 8192        conv5_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 60, 60, 2048) 0           conv5_2_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_162[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 60, 60, 2048) 0           add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_reduce (Conv2D)     (None, 60, 60, 512)  1048576     activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_reduce_bn (BatchNor (None, 60, 60, 512)  2048        conv5_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 60, 60, 512)  0           conv5_3_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_48 (ZeroPadding2 (None, 68, 68, 512)  0           activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_3x3 (Conv2D)            (None, 60, 60, 512)  2359296     zero_padding2d_48[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_3x3_bn (BatchNormalizat (None, 60, 60, 512)  2048        conv5_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 60, 60, 512)  0           conv5_3_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_increase (Conv2D)   (None, 60, 60, 2048) 1048576     activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_increase_bn (BatchN (None, 60, 60, 2048) 8192        conv5_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 60, 60, 2048) 0           conv5_3_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_165[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 60, 60, 2048) 0           add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_12 (AveragePo (None, 6, 6, 2048)   0           activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_11 (AveragePo (None, 3, 3, 2048)   0           activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_10 (AveragePo (None, 2, 2, 2048)   0           activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 1, 1, 2048)   0           activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool6_conv (Conv2D)     (None, 6, 6, 512)    1048576     average_pooling2d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool3_conv (Conv2D)     (None, 3, 3, 512)    1048576     average_pooling2d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool2_conv (Conv2D)     (None, 2, 2, 512)    1048576     average_pooling2d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool1_conv (Conv2D)     (None, 1, 1, 512)    1048576     average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool6_conv_bn (BatchNor (None, 6, 6, 512)    2048        conv5_3_pool6_conv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool3_conv_bn (BatchNor (None, 3, 3, 512)    2048        conv5_3_pool3_conv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool2_conv_bn (BatchNor (None, 2, 2, 512)    2048        conv5_3_pool2_conv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool1_conv_bn (BatchNor (None, 1, 1, 512)    2048        conv5_3_pool1_conv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 6, 6, 512)    0           conv5_3_pool6_conv_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 3, 3, 512)    0           conv5_3_pool3_conv_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 2, 2, 512)    0           conv5_3_pool2_conv_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 1, 1, 512)    0           conv5_3_pool1_conv_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "interp_14 (Interp)              (None, 60, 60, 512)  0           activation_172[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "interp_13 (Interp)              (None, 60, 60, 512)  0           activation_171[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "interp_12 (Interp)              (None, 60, 60, 512)  0           activation_170[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "interp_11 (Interp)              (None, 60, 60, 512)  0           activation_169[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 60, 60, 4096) 0           activation_168[0][0]             \n",
      "                                                                 interp_14[0][0]                  \n",
      "                                                                 interp_13[0][0]                  \n",
      "                                                                 interp_12[0][0]                  \n",
      "                                                                 interp_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv5_4 (Conv2D)                (None, 60, 60, 512)  18874368    concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_4_bn (BatchNormalization) (None, 60, 60, 512)  2048        conv5_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 60, 60, 512)  0           conv5_4_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 60, 60, 512)  0           activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv6 (Conv2D)                  (None, 60, 60, 51)   26163       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "interp_15 (Interp)              (None, 473, 473, 51) 0           conv6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 223729, 51)   0           interp_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 223729, 51)   0           reshape_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 46,790,387\n",
      "Trainable params: 26,163\n",
      "Non-trainable params: 46,764,224\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 367/367 [00:01<00:00, 235.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset verified! \n",
      "Starting Epoch  0\n",
      "WARNING:tensorflow:From c:\\users\\vash\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From c:\\users\\vash\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/512 [========>.....................] - ETA: 2:41:09 - loss: 4.6006 - accuracy: 0.035 - ETA: 2:08:18 - loss: 4.3377 - accuracy: 0.038 - ETA: 1:57:03 - loss: 4.0024 - accuracy: 0.073 - ETA: 1:51:21 - loss: 3.7998 - accuracy: 0.088 - ETA: 1:48:00 - loss: 3.6291 - accuracy: 0.108 - ETA: 1:45:44 - loss: 3.4299 - accuracy: 0.154 - ETA: 1:44:11 - loss: 3.2689 - accuracy: 0.190 - ETA: 1:42:57 - loss: 3.1409 - accuracy: 0.213 - ETA: 1:41:50 - loss: 3.0173 - accuracy: 0.237 - ETA: 1:40:56 - loss: 2.9103 - accuracy: 0.266 - ETA: 1:40:16 - loss: 2.7880 - accuracy: 0.299 - ETA: 1:39:38 - loss: 2.6891 - accuracy: 0.321 - ETA: 1:39:02 - loss: 2.6036 - accuracy: 0.339 - ETA: 1:38:29 - loss: 2.5149 - accuracy: 0.360 - ETA: 1:37:59 - loss: 2.4294 - accuracy: 0.385 - ETA: 1:37:36 - loss: 2.3539 - accuracy: 0.402 - ETA: 1:37:13 - loss: 2.2819 - accuracy: 0.422 - ETA: 1:36:54 - loss: 2.2343 - accuracy: 0.434 - ETA: 1:36:36 - loss: 2.1862 - accuracy: 0.444 - ETA: 1:36:24 - loss: 2.1338 - accuracy: 0.454 - ETA: 1:36:02 - loss: 2.1009 - accuracy: 0.460 - ETA: 1:35:41 - loss: 2.0682 - accuracy: 0.469 - ETA: 1:35:23 - loss: 2.0312 - accuracy: 0.480 - ETA: 1:35:03 - loss: 1.9843 - accuracy: 0.493 - ETA: 1:34:45 - loss: 1.9626 - accuracy: 0.496 - ETA: 1:34:28 - loss: 1.9383 - accuracy: 0.501 - ETA: 1:34:13 - loss: 1.9097 - accuracy: 0.506 - ETA: 1:33:59 - loss: 1.8670 - accuracy: 0.518 - ETA: 1:33:45 - loss: 1.8329 - accuracy: 0.527 - ETA: 1:33:30 - loss: 1.8114 - accuracy: 0.533 - ETA: 1:33:17 - loss: 1.7894 - accuracy: 0.538 - ETA: 1:33:01 - loss: 1.7537 - accuracy: 0.548 - ETA: 1:32:48 - loss: 1.7267 - accuracy: 0.555 - ETA: 1:32:36 - loss: 1.6992 - accuracy: 0.562 - ETA: 1:32:20 - loss: 1.6812 - accuracy: 0.566 - ETA: 1:32:04 - loss: 1.6502 - accuracy: 0.575 - ETA: 1:31:50 - loss: 1.6358 - accuracy: 0.579 - ETA: 1:31:35 - loss: 1.6092 - accuracy: 0.586 - ETA: 1:31:22 - loss: 1.5884 - accuracy: 0.591 - ETA: 1:31:10 - loss: 1.5679 - accuracy: 0.596 - ETA: 1:30:56 - loss: 1.5514 - accuracy: 0.599 - ETA: 1:30:44 - loss: 1.5365 - accuracy: 0.601 - ETA: 1:30:31 - loss: 1.5143 - accuracy: 0.608 - ETA: 1:30:18 - loss: 1.5023 - accuracy: 0.610 - ETA: 1:30:04 - loss: 1.4870 - accuracy: 0.613 - ETA: 1:29:51 - loss: 1.4709 - accuracy: 0.618 - ETA: 1:29:37 - loss: 1.4521 - accuracy: 0.623 - ETA: 1:29:23 - loss: 1.4397 - accuracy: 0.626 - ETA: 1:29:10 - loss: 1.4226 - accuracy: 0.630 - ETA: 1:28:58 - loss: 1.4076 - accuracy: 0.634 - ETA: 1:28:45 - loss: 1.3933 - accuracy: 0.638 - ETA: 1:28:33 - loss: 1.3774 - accuracy: 0.642 - ETA: 1:28:23 - loss: 1.3680 - accuracy: 0.643 - ETA: 1:28:13 - loss: 1.3578 - accuracy: 0.645 - ETA: 1:28:01 - loss: 1.3438 - accuracy: 0.650 - ETA: 1:27:49 - loss: 1.3378 - accuracy: 0.651 - ETA: 1:27:36 - loss: 1.3281 - accuracy: 0.653 - ETA: 1:27:24 - loss: 1.3178 - accuracy: 0.655 - ETA: 1:27:10 - loss: 1.3050 - accuracy: 0.659 - ETA: 1:26:57 - loss: 1.2955 - accuracy: 0.661 - ETA: 1:26:44 - loss: 1.2917 - accuracy: 0.662 - ETA: 1:26:33 - loss: 1.2785 - accuracy: 0.666 - ETA: 1:26:20 - loss: 1.2653 - accuracy: 0.670 - ETA: 1:26:07 - loss: 1.2554 - accuracy: 0.672 - ETA: 1:25:53 - loss: 1.2436 - accuracy: 0.675 - ETA: 1:25:39 - loss: 1.2355 - accuracy: 0.677 - ETA: 1:25:25 - loss: 1.2264 - accuracy: 0.679 - ETA: 1:25:11 - loss: 1.2152 - accuracy: 0.682 - ETA: 1:24:56 - loss: 1.2095 - accuracy: 0.683 - ETA: 1:24:43 - loss: 1.2015 - accuracy: 0.685 - ETA: 1:24:30 - loss: 1.1950 - accuracy: 0.686 - ETA: 1:24:17 - loss: 1.1897 - accuracy: 0.687 - ETA: 1:24:04 - loss: 1.1811 - accuracy: 0.690 - ETA: 1:23:51 - loss: 1.1723 - accuracy: 0.692 - ETA: 1:23:38 - loss: 1.1642 - accuracy: 0.694 - ETA: 1:23:25 - loss: 1.1560 - accuracy: 0.696 - ETA: 1:23:11 - loss: 1.1498 - accuracy: 0.697 - ETA: 1:22:59 - loss: 1.1448 - accuracy: 0.698 - ETA: 1:22:46 - loss: 1.1367 - accuracy: 0.700 - ETA: 1:22:33 - loss: 1.1292 - accuracy: 0.702 - ETA: 1:22:20 - loss: 1.1245 - accuracy: 0.703 - ETA: 1:22:07 - loss: 1.1212 - accuracy: 0.703 - ETA: 1:21:54 - loss: 1.1137 - accuracy: 0.705 - ETA: 1:21:41 - loss: 1.1079 - accuracy: 0.706 - ETA: 1:21:28 - loss: 1.1007 - accuracy: 0.708 - ETA: 1:21:16 - loss: 1.0970 - accuracy: 0.709 - ETA: 1:21:04 - loss: 1.0918 - accuracy: 0.710 - ETA: 1:20:51 - loss: 1.0855 - accuracy: 0.712 - ETA: 1:20:39 - loss: 1.0816 - accuracy: 0.712 - ETA: 1:20:26 - loss: 1.0753 - accuracy: 0.714 - ETA: 1:20:14 - loss: 1.0704 - accuracy: 0.715 - ETA: 1:20:01 - loss: 1.0652 - accuracy: 0.716 - ETA: 1:19:49 - loss: 1.0624 - accuracy: 0.716 - ETA: 1:19:37 - loss: 1.0575 - accuracy: 0.717 - ETA: 1:19:24 - loss: 1.0524 - accuracy: 0.719 - ETA: 1:19:12 - loss: 1.0485 - accuracy: 0.719 - ETA: 1:19:00 - loss: 1.0439 - accuracy: 0.721 - ETA: 1:18:48 - loss: 1.0415 - accuracy: 0.721 - ETA: 1:18:36 - loss: 1.0350 - accuracy: 0.722 - ETA: 1:18:24 - loss: 1.0303 - accuracy: 0.723 - ETA: 1:18:12 - loss: 1.0255 - accuracy: 0.725 - ETA: 1:18:00 - loss: 1.0206 - accuracy: 0.726 - ETA: 1:17:48 - loss: 1.0163 - accuracy: 0.727 - ETA: 1:17:36 - loss: 1.0152 - accuracy: 0.727 - ETA: 1:17:24 - loss: 1.0137 - accuracy: 0.727 - ETA: 1:17:12 - loss: 1.0090 - accuracy: 0.728 - ETA: 1:17:00 - loss: 1.0067 - accuracy: 0.728 - ETA: 1:16:48 - loss: 1.0031 - accuracy: 0.729 - ETA: 1:16:37 - loss: 0.9982 - accuracy: 0.730 - ETA: 1:16:24 - loss: 0.9974 - accuracy: 0.730 - ETA: 1:16:13 - loss: 0.9935 - accuracy: 0.731 - ETA: 1:16:01 - loss: 0.9889 - accuracy: 0.732 - ETA: 1:15:48 - loss: 0.9846 - accuracy: 0.733 - ETA: 1:15:37 - loss: 0.9825 - accuracy: 0.734 - ETA: 1:15:24 - loss: 0.9804 - accuracy: 0.734 - ETA: 1:15:12 - loss: 0.9764 - accuracy: 0.735 - ETA: 1:15:01 - loss: 0.9721 - accuracy: 0.736 - ETA: 1:14:49 - loss: 0.9718 - accuracy: 0.736 - ETA: 1:14:37 - loss: 0.9679 - accuracy: 0.737 - ETA: 1:14:25 - loss: 0.9653 - accuracy: 0.737 - ETA: 1:14:13 - loss: 0.9635 - accuracy: 0.737 - ETA: 1:14:01 - loss: 0.9628 - accuracy: 0.737 - ETA: 1:13:49 - loss: 0.9587 - accuracy: 0.738 - ETA: 1:13:38 - loss: 0.9561 - accuracy: 0.739 - ETA: 1:13:26 - loss: 0.9523 - accuracy: 0.740 - ETA: 1:13:15 - loss: 0.9496 - accuracy: 0.741 - ETA: 1:13:03 - loss: 0.9465 - accuracy: 0.741 - ETA: 1:12:51 - loss: 0.9446 - accuracy: 0.741 - ETA: 1:12:40 - loss: 0.9415 - accuracy: 0.742 - ETA: 1:12:28 - loss: 0.9378 - accuracy: 0.743 - ETA: 1:12:17 - loss: 0.9340 - accuracy: 0.744 - ETA: 1:12:05 - loss: 0.9298 - accuracy: 0.746 - ETA: 1:11:53 - loss: 0.9293 - accuracy: 0.745 - ETA: 1:11:42 - loss: 0.9266 - accuracy: 0.746 - ETA: 1:11:30 - loss: 0.9219 - accuracy: 0.747 - ETA: 1:11:18 - loss: 0.9182 - accuracy: 0.748 - ETA: 1:11:06 - loss: 0.9146 - accuracy: 0.749 - ETA: 1:10:55 - loss: 0.9143 - accuracy: 0.749 - ETA: 1:10:43 - loss: 0.9112 - accuracy: 0.750 - ETA: 1:10:32 - loss: 0.9089 - accuracy: 0.750 - ETA: 1:10:20 - loss: 0.9065 - accuracy: 0.751 - ETA: 1:10:08 - loss: 0.9062 - accuracy: 0.751 - ETA: 1:09:57 - loss: 0.9051 - accuracy: 0.751 - ETA: 1:09:45 - loss: 0.9021 - accuracy: 0.751 - ETA: 1:09:34 - loss: 0.9002 - accuracy: 0.752 - ETA: 1:09:22 - loss: 0.8987 - accuracy: 0.752 - ETA: 1:09:10 - loss: 0.8960 - accuracy: 0.753 - ETA: 1:08:59 - loss: 0.8926 - accuracy: 0.754 - ETA: 1:08:47 - loss: 0.8897 - accuracy: 0.754 - ETA: 1:08:35 - loss: 0.8874 - accuracy: 0.755 - ETA: 1:08:24 - loss: 0.8849 - accuracy: 0.755 - ETA: 1:08:12 - loss: 0.8813 - accuracy: 0.756 - ETA: 1:08:00 - loss: 0.8794 - accuracy: 0.757 - ETA: 1:07:49 - loss: 0.8765 - accuracy: 0.757 - ETA: 1:07:37 - loss: 0.8731 - accuracy: 0.758 - ETA: 1:07:25 - loss: 0.8707 - accuracy: 0.759 - ETA: 1:07:14 - loss: 0.8684 - accuracy: 0.759 - ETA: 1:07:02 - loss: 0.8667 - accuracy: 0.759 - ETA: 1:06:51 - loss: 0.8645 - accuracy: 0.760 - ETA: 1:06:39 - loss: 0.8616 - accuracy: 0.760 - ETA: 1:06:27 - loss: 0.8602 - accuracy: 0.761 - ETA: 1:06:16 - loss: 0.8579 - accuracy: 0.761 - ETA: 1:06:04 - loss: 0.8564 - accuracy: 0.761 - ETA: 1:05:53 - loss: 0.8552 - accuracy: 0.762 - ETA: 1:05:41 - loss: 0.8524 - accuracy: 0.762 - ETA: 1:05:30 - loss: 0.8502 - accuracy: 0.763 - ETA: 1:05:18 - loss: 0.8475 - accuracy: 0.764 - ETA: 1:05:06 - loss: 0.8470 - accuracy: 0.764 - ETA: 1:04:55 - loss: 0.8449 - accuracy: 0.764 - ETA: 1:04:44 - loss: 0.8422 - accuracy: 0.7655347/512 [===================>..........] - ETA: 1:04:32 - loss: 0.8413 - accuracy: 0.765 - ETA: 1:04:21 - loss: 0.8391 - accuracy: 0.766 - ETA: 1:04:09 - loss: 0.8380 - accuracy: 0.766 - ETA: 1:03:58 - loss: 0.8366 - accuracy: 0.766 - ETA: 1:03:46 - loss: 0.8337 - accuracy: 0.767 - ETA: 1:03:34 - loss: 0.8313 - accuracy: 0.768 - ETA: 1:03:23 - loss: 0.8302 - accuracy: 0.768 - ETA: 1:03:12 - loss: 0.8275 - accuracy: 0.769 - ETA: 1:03:01 - loss: 0.8245 - accuracy: 0.769 - ETA: 1:02:49 - loss: 0.8226 - accuracy: 0.770 - ETA: 1:02:38 - loss: 0.8210 - accuracy: 0.770 - ETA: 1:02:27 - loss: 0.8193 - accuracy: 0.771 - ETA: 1:02:16 - loss: 0.8190 - accuracy: 0.770 - ETA: 1:02:05 - loss: 0.8168 - accuracy: 0.771 - ETA: 1:01:53 - loss: 0.8151 - accuracy: 0.771 - ETA: 1:01:42 - loss: 0.8139 - accuracy: 0.772 - ETA: 1:01:30 - loss: 0.8119 - accuracy: 0.772 - ETA: 1:01:19 - loss: 0.8094 - accuracy: 0.773 - ETA: 1:01:07 - loss: 0.8077 - accuracy: 0.773 - ETA: 1:00:56 - loss: 0.8059 - accuracy: 0.773 - ETA: 1:00:45 - loss: 0.8047 - accuracy: 0.774 - ETA: 1:00:33 - loss: 0.8024 - accuracy: 0.774 - ETA: 1:00:22 - loss: 0.8003 - accuracy: 0.775 - ETA: 1:00:11 - loss: 0.8006 - accuracy: 0.775 - ETA: 1:00:00 - loss: 0.7990 - accuracy: 0.775 - ETA: 59:48 - loss: 0.7971 - accuracy: 0.7757  - ETA: 59:37 - loss: 0.7951 - accuracy: 0.776 - ETA: 59:26 - loss: 0.7931 - accuracy: 0.776 - ETA: 59:15 - loss: 0.7910 - accuracy: 0.777 - ETA: 59:03 - loss: 0.7890 - accuracy: 0.777 - ETA: 58:52 - loss: 0.7872 - accuracy: 0.778 - ETA: 58:41 - loss: 0.7868 - accuracy: 0.778 - ETA: 58:30 - loss: 0.7853 - accuracy: 0.778 - ETA: 58:19 - loss: 0.7836 - accuracy: 0.778 - ETA: 58:08 - loss: 0.7841 - accuracy: 0.778 - ETA: 57:56 - loss: 0.7829 - accuracy: 0.778 - ETA: 57:45 - loss: 0.7819 - accuracy: 0.779 - ETA: 57:34 - loss: 0.7812 - accuracy: 0.779 - ETA: 57:22 - loss: 0.7797 - accuracy: 0.779 - ETA: 57:11 - loss: 0.7794 - accuracy: 0.779 - ETA: 57:00 - loss: 0.7778 - accuracy: 0.779 - ETA: 56:49 - loss: 0.7763 - accuracy: 0.780 - ETA: 56:37 - loss: 0.7757 - accuracy: 0.780 - ETA: 56:26 - loss: 0.7770 - accuracy: 0.779 - ETA: 56:15 - loss: 0.7751 - accuracy: 0.780 - ETA: 56:03 - loss: 0.7733 - accuracy: 0.780 - ETA: 55:52 - loss: 0.7717 - accuracy: 0.781 - ETA: 55:41 - loss: 0.7702 - accuracy: 0.781 - ETA: 55:29 - loss: 0.7689 - accuracy: 0.782 - ETA: 55:18 - loss: 0.7672 - accuracy: 0.782 - ETA: 55:06 - loss: 0.7668 - accuracy: 0.782 - ETA: 54:55 - loss: 0.7652 - accuracy: 0.783 - ETA: 54:44 - loss: 0.7640 - accuracy: 0.783 - ETA: 54:33 - loss: 0.7625 - accuracy: 0.783 - ETA: 54:21 - loss: 0.7617 - accuracy: 0.783 - ETA: 54:10 - loss: 0.7596 - accuracy: 0.784 - ETA: 53:59 - loss: 0.7578 - accuracy: 0.784 - ETA: 53:47 - loss: 0.7569 - accuracy: 0.784 - ETA: 53:36 - loss: 0.7552 - accuracy: 0.785 - ETA: 53:25 - loss: 0.7548 - accuracy: 0.785 - ETA: 53:14 - loss: 0.7536 - accuracy: 0.785 - ETA: 53:03 - loss: 0.7522 - accuracy: 0.786 - ETA: 52:52 - loss: 0.7509 - accuracy: 0.786 - ETA: 52:41 - loss: 0.7507 - accuracy: 0.786 - ETA: 52:29 - loss: 0.7502 - accuracy: 0.786 - ETA: 52:18 - loss: 0.7488 - accuracy: 0.786 - ETA: 52:07 - loss: 0.7480 - accuracy: 0.786 - ETA: 51:56 - loss: 0.7473 - accuracy: 0.786 - ETA: 51:45 - loss: 0.7453 - accuracy: 0.787 - ETA: 51:33 - loss: 0.7453 - accuracy: 0.787 - ETA: 51:22 - loss: 0.7446 - accuracy: 0.787 - ETA: 51:11 - loss: 0.7431 - accuracy: 0.787 - ETA: 51:00 - loss: 0.7416 - accuracy: 0.788 - ETA: 50:49 - loss: 0.7418 - accuracy: 0.788 - ETA: 50:38 - loss: 0.7406 - accuracy: 0.788 - ETA: 50:27 - loss: 0.7390 - accuracy: 0.788 - ETA: 50:16 - loss: 0.7376 - accuracy: 0.789 - ETA: 50:05 - loss: 0.7366 - accuracy: 0.789 - ETA: 49:54 - loss: 0.7354 - accuracy: 0.789 - ETA: 49:43 - loss: 0.7342 - accuracy: 0.789 - ETA: 49:32 - loss: 0.7325 - accuracy: 0.790 - ETA: 49:20 - loss: 0.7312 - accuracy: 0.790 - ETA: 49:09 - loss: 0.7303 - accuracy: 0.790 - ETA: 48:57 - loss: 0.7299 - accuracy: 0.790 - ETA: 48:46 - loss: 0.7288 - accuracy: 0.791 - ETA: 48:34 - loss: 0.7281 - accuracy: 0.791 - ETA: 48:23 - loss: 0.7268 - accuracy: 0.791 - ETA: 48:12 - loss: 0.7257 - accuracy: 0.791 - ETA: 48:00 - loss: 0.7242 - accuracy: 0.792 - ETA: 47:49 - loss: 0.7231 - accuracy: 0.792 - ETA: 47:38 - loss: 0.7224 - accuracy: 0.792 - ETA: 47:26 - loss: 0.7213 - accuracy: 0.792 - ETA: 47:15 - loss: 0.7202 - accuracy: 0.793 - ETA: 47:04 - loss: 0.7190 - accuracy: 0.793 - ETA: 46:53 - loss: 0.7186 - accuracy: 0.793 - ETA: 46:41 - loss: 0.7182 - accuracy: 0.793 - ETA: 46:30 - loss: 0.7170 - accuracy: 0.793 - ETA: 46:18 - loss: 0.7161 - accuracy: 0.794 - ETA: 46:07 - loss: 0.7148 - accuracy: 0.794 - ETA: 45:56 - loss: 0.7142 - accuracy: 0.794 - ETA: 45:44 - loss: 0.7131 - accuracy: 0.794 - ETA: 45:33 - loss: 0.7119 - accuracy: 0.795 - ETA: 45:21 - loss: 0.7118 - accuracy: 0.795 - ETA: 45:10 - loss: 0.7112 - accuracy: 0.795 - ETA: 44:58 - loss: 0.7099 - accuracy: 0.795 - ETA: 44:47 - loss: 0.7099 - accuracy: 0.795 - ETA: 44:35 - loss: 0.7092 - accuracy: 0.795 - ETA: 44:23 - loss: 0.7079 - accuracy: 0.795 - ETA: 44:12 - loss: 0.7069 - accuracy: 0.796 - ETA: 44:00 - loss: 0.7064 - accuracy: 0.796 - ETA: 43:49 - loss: 0.7055 - accuracy: 0.796 - ETA: 43:37 - loss: 0.7050 - accuracy: 0.796 - ETA: 43:26 - loss: 0.7043 - accuracy: 0.796 - ETA: 43:14 - loss: 0.7034 - accuracy: 0.796 - ETA: 43:03 - loss: 0.7024 - accuracy: 0.797 - ETA: 42:52 - loss: 0.7014 - accuracy: 0.797 - ETA: 42:41 - loss: 0.7004 - accuracy: 0.797 - ETA: 42:29 - loss: 0.7002 - accuracy: 0.797 - ETA: 42:18 - loss: 0.6997 - accuracy: 0.797 - ETA: 42:07 - loss: 0.6999 - accuracy: 0.797 - ETA: 41:55 - loss: 0.6993 - accuracy: 0.797 - ETA: 41:44 - loss: 0.6982 - accuracy: 0.797 - ETA: 41:33 - loss: 0.6974 - accuracy: 0.798 - ETA: 41:21 - loss: 0.6976 - accuracy: 0.797 - ETA: 41:10 - loss: 0.6965 - accuracy: 0.798 - ETA: 40:59 - loss: 0.6954 - accuracy: 0.798 - ETA: 40:47 - loss: 0.6942 - accuracy: 0.798 - ETA: 40:36 - loss: 0.6959 - accuracy: 0.798 - ETA: 40:25 - loss: 0.6947 - accuracy: 0.798 - ETA: 40:13 - loss: 0.6940 - accuracy: 0.798 - ETA: 40:02 - loss: 0.6934 - accuracy: 0.798 - ETA: 39:51 - loss: 0.6933 - accuracy: 0.798 - ETA: 39:39 - loss: 0.6927 - accuracy: 0.799 - ETA: 39:28 - loss: 0.6924 - accuracy: 0.799 - ETA: 39:17 - loss: 0.6928 - accuracy: 0.798 - ETA: 39:05 - loss: 0.6918 - accuracy: 0.799 - ETA: 38:54 - loss: 0.6907 - accuracy: 0.799 - ETA: 38:43 - loss: 0.6898 - accuracy: 0.799 - ETA: 38:32 - loss: 0.6887 - accuracy: 0.799 - ETA: 38:20 - loss: 0.6887 - accuracy: 0.799 - ETA: 38:09 - loss: 0.6881 - accuracy: 0.800 - ETA: 37:58 - loss: 0.6871 - accuracy: 0.800 - ETA: 37:47 - loss: 0.6864 - accuracy: 0.800 - ETA: 37:35 - loss: 0.6856 - accuracy: 0.800 - ETA: 37:24 - loss: 0.6846 - accuracy: 0.800 - ETA: 37:13 - loss: 0.6841 - accuracy: 0.801 - ETA: 37:01 - loss: 0.6836 - accuracy: 0.801 - ETA: 36:50 - loss: 0.6826 - accuracy: 0.801 - ETA: 36:39 - loss: 0.6816 - accuracy: 0.801 - ETA: 36:27 - loss: 0.6803 - accuracy: 0.802 - ETA: 36:16 - loss: 0.6800 - accuracy: 0.802 - ETA: 36:04 - loss: 0.6802 - accuracy: 0.802 - ETA: 35:53 - loss: 0.6799 - accuracy: 0.802 - ETA: 35:42 - loss: 0.6791 - accuracy: 0.802 - ETA: 35:30 - loss: 0.6793 - accuracy: 0.802 - ETA: 35:19 - loss: 0.6796 - accuracy: 0.802 - ETA: 35:07 - loss: 0.6787 - accuracy: 0.802 - ETA: 34:56 - loss: 0.6783 - accuracy: 0.802 - ETA: 34:45 - loss: 0.6782 - accuracy: 0.802 - ETA: 34:33 - loss: 0.6774 - accuracy: 0.802 - ETA: 34:22 - loss: 0.6765 - accuracy: 0.802 - ETA: 34:11 - loss: 0.6757 - accuracy: 0.802 - ETA: 33:59 - loss: 0.6754 - accuracy: 0.802 - ETA: 33:48 - loss: 0.6748 - accuracy: 0.803 - ETA: 33:36 - loss: 0.6740 - accuracy: 0.803 - ETA: 33:25 - loss: 0.6734 - accuracy: 0.803 - ETA: 33:14 - loss: 0.6727 - accuracy: 0.803 - ETA: 33:02 - loss: 0.6716 - accuracy: 0.803 - ETA: 32:51 - loss: 0.6707 - accuracy: 0.804 - ETA: 32:39 - loss: 0.6696 - accuracy: 0.804 - ETA: 32:28 - loss: 0.6698 - accuracy: 0.804 - ETA: 32:17 - loss: 0.6687 - accuracy: 0.804 - ETA: 32:05 - loss: 0.6681 - accuracy: 0.804 - ETA: 31:54 - loss: 0.6677 - accuracy: 0.804 - ETA: 31:43 - loss: 0.6671 - accuracy: 0.804 - ETA: 31:31 - loss: 0.6674 - accuracy: 0.804 - ETA: 31:20 - loss: 0.6666 - accuracy: 0.8049"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - ETA: 31:09 - loss: 0.6658 - accuracy: 0.805 - ETA: 30:57 - loss: 0.6654 - accuracy: 0.805 - ETA: 30:46 - loss: 0.6644 - accuracy: 0.805 - ETA: 30:34 - loss: 0.6640 - accuracy: 0.805 - ETA: 30:23 - loss: 0.6635 - accuracy: 0.805 - ETA: 30:12 - loss: 0.6627 - accuracy: 0.805 - ETA: 30:00 - loss: 0.6618 - accuracy: 0.806 - ETA: 29:49 - loss: 0.6618 - accuracy: 0.806 - ETA: 29:37 - loss: 0.6610 - accuracy: 0.806 - ETA: 29:26 - loss: 0.6611 - accuracy: 0.806 - ETA: 29:15 - loss: 0.6600 - accuracy: 0.806 - ETA: 29:03 - loss: 0.6592 - accuracy: 0.806 - ETA: 28:52 - loss: 0.6585 - accuracy: 0.807 - ETA: 28:40 - loss: 0.6577 - accuracy: 0.807 - ETA: 28:29 - loss: 0.6567 - accuracy: 0.807 - ETA: 28:18 - loss: 0.6557 - accuracy: 0.807 - ETA: 28:06 - loss: 0.6556 - accuracy: 0.807 - ETA: 27:55 - loss: 0.6547 - accuracy: 0.808 - ETA: 27:43 - loss: 0.6546 - accuracy: 0.807 - ETA: 27:32 - loss: 0.6539 - accuracy: 0.808 - ETA: 27:20 - loss: 0.6534 - accuracy: 0.808 - ETA: 27:09 - loss: 0.6534 - accuracy: 0.808 - ETA: 26:58 - loss: 0.6528 - accuracy: 0.808 - ETA: 26:46 - loss: 0.6521 - accuracy: 0.808 - ETA: 26:35 - loss: 0.6512 - accuracy: 0.808 - ETA: 26:23 - loss: 0.6502 - accuracy: 0.809 - ETA: 26:12 - loss: 0.6499 - accuracy: 0.809 - ETA: 26:01 - loss: 0.6495 - accuracy: 0.809 - ETA: 25:49 - loss: 0.6490 - accuracy: 0.809 - ETA: 25:38 - loss: 0.6486 - accuracy: 0.809 - ETA: 25:26 - loss: 0.6479 - accuracy: 0.809 - ETA: 25:15 - loss: 0.6473 - accuracy: 0.809 - ETA: 25:04 - loss: 0.6468 - accuracy: 0.809 - ETA: 24:52 - loss: 0.6462 - accuracy: 0.810 - ETA: 24:41 - loss: 0.6453 - accuracy: 0.810 - ETA: 24:29 - loss: 0.6446 - accuracy: 0.810 - ETA: 24:18 - loss: 0.6439 - accuracy: 0.810 - ETA: 24:07 - loss: 0.6439 - accuracy: 0.810 - ETA: 23:55 - loss: 0.6434 - accuracy: 0.810 - ETA: 23:44 - loss: 0.6430 - accuracy: 0.810 - ETA: 23:33 - loss: 0.6432 - accuracy: 0.810 - ETA: 23:21 - loss: 0.6431 - accuracy: 0.810 - ETA: 23:10 - loss: 0.6426 - accuracy: 0.810 - ETA: 22:58 - loss: 0.6419 - accuracy: 0.810 - ETA: 22:47 - loss: 0.6422 - accuracy: 0.810 - ETA: 22:36 - loss: 0.6417 - accuracy: 0.810 - ETA: 22:24 - loss: 0.6415 - accuracy: 0.810 - ETA: 22:13 - loss: 0.6407 - accuracy: 0.811 - ETA: 22:01 - loss: 0.6401 - accuracy: 0.811 - ETA: 21:50 - loss: 0.6397 - accuracy: 0.811 - ETA: 21:39 - loss: 0.6395 - accuracy: 0.811 - ETA: 21:27 - loss: 0.6387 - accuracy: 0.811 - ETA: 21:16 - loss: 0.6381 - accuracy: 0.811 - ETA: 21:05 - loss: 0.6375 - accuracy: 0.811 - ETA: 20:53 - loss: 0.6371 - accuracy: 0.812 - ETA: 20:42 - loss: 0.6362 - accuracy: 0.812 - ETA: 20:30 - loss: 0.6362 - accuracy: 0.812 - ETA: 20:19 - loss: 0.6353 - accuracy: 0.812 - ETA: 20:07 - loss: 0.6348 - accuracy: 0.812 - ETA: 19:56 - loss: 0.6342 - accuracy: 0.812 - ETA: 19:45 - loss: 0.6337 - accuracy: 0.812 - ETA: 19:33 - loss: 0.6335 - accuracy: 0.812 - ETA: 19:22 - loss: 0.6326 - accuracy: 0.813 - ETA: 19:11 - loss: 0.6327 - accuracy: 0.813 - ETA: 18:59 - loss: 0.6323 - accuracy: 0.813 - ETA: 18:48 - loss: 0.6319 - accuracy: 0.813 - ETA: 18:36 - loss: 0.6313 - accuracy: 0.813 - ETA: 18:25 - loss: 0.6311 - accuracy: 0.813 - ETA: 18:14 - loss: 0.6303 - accuracy: 0.813 - ETA: 18:02 - loss: 0.6299 - accuracy: 0.813 - ETA: 17:51 - loss: 0.6292 - accuracy: 0.814 - ETA: 17:39 - loss: 0.6285 - accuracy: 0.814 - ETA: 17:28 - loss: 0.6282 - accuracy: 0.814 - ETA: 17:17 - loss: 0.6277 - accuracy: 0.814 - ETA: 17:05 - loss: 0.6269 - accuracy: 0.814 - ETA: 16:54 - loss: 0.6271 - accuracy: 0.814 - ETA: 16:43 - loss: 0.6267 - accuracy: 0.814 - ETA: 16:31 - loss: 0.6262 - accuracy: 0.814 - ETA: 16:20 - loss: 0.6255 - accuracy: 0.814 - ETA: 16:08 - loss: 0.6251 - accuracy: 0.815 - ETA: 15:57 - loss: 0.6253 - accuracy: 0.814 - ETA: 15:45 - loss: 0.6245 - accuracy: 0.815 - ETA: 15:34 - loss: 0.6237 - accuracy: 0.815 - ETA: 15:23 - loss: 0.6232 - accuracy: 0.815 - ETA: 15:11 - loss: 0.6225 - accuracy: 0.815 - ETA: 15:00 - loss: 0.6223 - accuracy: 0.815 - ETA: 14:49 - loss: 0.6218 - accuracy: 0.815 - ETA: 14:37 - loss: 0.6211 - accuracy: 0.816 - ETA: 14:26 - loss: 0.6209 - accuracy: 0.816 - ETA: 14:14 - loss: 0.6204 - accuracy: 0.816 - ETA: 14:03 - loss: 0.6202 - accuracy: 0.816 - ETA: 13:52 - loss: 0.6200 - accuracy: 0.816 - ETA: 13:40 - loss: 0.6194 - accuracy: 0.816 - ETA: 13:29 - loss: 0.6190 - accuracy: 0.816 - ETA: 13:17 - loss: 0.6185 - accuracy: 0.816 - ETA: 13:06 - loss: 0.6180 - accuracy: 0.816 - ETA: 12:55 - loss: 0.6175 - accuracy: 0.816 - ETA: 12:43 - loss: 0.6173 - accuracy: 0.816 - ETA: 12:32 - loss: 0.6167 - accuracy: 0.817 - ETA: 12:21 - loss: 0.6161 - accuracy: 0.817 - ETA: 12:09 - loss: 0.6157 - accuracy: 0.817 - ETA: 11:58 - loss: 0.6157 - accuracy: 0.817 - ETA: 11:47 - loss: 0.6151 - accuracy: 0.817 - ETA: 11:35 - loss: 0.6146 - accuracy: 0.817 - ETA: 11:24 - loss: 0.6141 - accuracy: 0.817 - ETA: 11:12 - loss: 0.6139 - accuracy: 0.817 - ETA: 11:01 - loss: 0.6134 - accuracy: 0.817 - ETA: 10:50 - loss: 0.6129 - accuracy: 0.818 - ETA: 10:38 - loss: 0.6129 - accuracy: 0.817 - ETA: 10:27 - loss: 0.6124 - accuracy: 0.818 - ETA: 10:15 - loss: 0.6120 - accuracy: 0.818 - ETA: 10:04 - loss: 0.6116 - accuracy: 0.818 - ETA: 9:53 - loss: 0.6114 - accuracy: 0.818 - ETA: 9:41 - loss: 0.6110 - accuracy: 0.81 - ETA: 9:30 - loss: 0.6105 - accuracy: 0.81 - ETA: 9:18 - loss: 0.6103 - accuracy: 0.81 - ETA: 9:07 - loss: 0.6101 - accuracy: 0.81 - ETA: 8:55 - loss: 0.6102 - accuracy: 0.81 - ETA: 8:44 - loss: 0.6096 - accuracy: 0.81 - ETA: 8:33 - loss: 0.6092 - accuracy: 0.81 - ETA: 8:21 - loss: 0.6087 - accuracy: 0.81 - ETA: 8:10 - loss: 0.6082 - accuracy: 0.81 - ETA: 7:59 - loss: 0.6077 - accuracy: 0.81 - ETA: 7:47 - loss: 0.6078 - accuracy: 0.81 - ETA: 7:36 - loss: 0.6078 - accuracy: 0.81 - ETA: 7:24 - loss: 0.6074 - accuracy: 0.81 - ETA: 7:13 - loss: 0.6074 - accuracy: 0.81 - ETA: 7:02 - loss: 0.6071 - accuracy: 0.81 - ETA: 6:50 - loss: 0.6065 - accuracy: 0.81 - ETA: 6:39 - loss: 0.6067 - accuracy: 0.81 - ETA: 6:27 - loss: 0.6063 - accuracy: 0.81 - ETA: 6:16 - loss: 0.6058 - accuracy: 0.81 - ETA: 6:05 - loss: 0.6053 - accuracy: 0.81 - ETA: 5:53 - loss: 0.6051 - accuracy: 0.81 - ETA: 5:42 - loss: 0.6051 - accuracy: 0.81 - ETA: 5:30 - loss: 0.6047 - accuracy: 0.81 - ETA: 5:19 - loss: 0.6043 - accuracy: 0.81 - ETA: 5:08 - loss: 0.6046 - accuracy: 0.81 - ETA: 4:56 - loss: 0.6041 - accuracy: 0.81 - ETA: 4:45 - loss: 0.6041 - accuracy: 0.81 - ETA: 4:33 - loss: 0.6040 - accuracy: 0.81 - ETA: 4:22 - loss: 0.6041 - accuracy: 0.81 - ETA: 4:11 - loss: 0.6036 - accuracy: 0.81 - ETA: 3:59 - loss: 0.6032 - accuracy: 0.81 - ETA: 3:48 - loss: 0.6028 - accuracy: 0.82 - ETA: 3:36 - loss: 0.6025 - accuracy: 0.82 - ETA: 3:25 - loss: 0.6022 - accuracy: 0.82 - ETA: 3:13 - loss: 0.6020 - accuracy: 0.82 - ETA: 3:02 - loss: 0.6016 - accuracy: 0.82 - ETA: 2:51 - loss: 0.6011 - accuracy: 0.82 - ETA: 2:39 - loss: 0.6005 - accuracy: 0.82 - ETA: 2:28 - loss: 0.5999 - accuracy: 0.82 - ETA: 2:16 - loss: 0.6002 - accuracy: 0.82 - ETA: 2:05 - loss: 0.5999 - accuracy: 0.82 - ETA: 1:54 - loss: 0.5991 - accuracy: 0.82 - ETA: 1:42 - loss: 0.5986 - accuracy: 0.82 - ETA: 1:31 - loss: 0.5980 - accuracy: 0.82 - ETA: 1:19 - loss: 0.5982 - accuracy: 0.82 - ETA: 1:08 - loss: 0.5978 - accuracy: 0.82 - ETA: 57s - loss: 0.5976 - accuracy: 0.8213 - ETA: 45s - loss: 0.5973 - accuracy: 0.821 - ETA: 34s - loss: 0.5976 - accuracy: 0.821 - ETA: 22s - loss: 0.5977 - accuracy: 0.821 - ETA: 11s - loss: 0.5973 - accuracy: 0.821 - 5843s 11s/step - loss: 0.5970 - accuracy: 0.8214\n",
      "Finished Epoch 0\n",
      "Starting Epoch  1\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/512 [========>.....................] - ETA: 1:47:40 - loss: 0.4166 - accuracy: 0.860 - ETA: 1:42:10 - loss: 0.4358 - accuracy: 0.856 - ETA: 1:40:19 - loss: 0.4236 - accuracy: 0.861 - ETA: 1:39:32 - loss: 0.4047 - accuracy: 0.865 - ETA: 1:38:46 - loss: 0.4207 - accuracy: 0.862 - ETA: 1:38:16 - loss: 0.4105 - accuracy: 0.864 - ETA: 1:37:44 - loss: 0.4184 - accuracy: 0.860 - ETA: 1:37:24 - loss: 0.4263 - accuracy: 0.858 - ETA: 1:37:05 - loss: 0.4171 - accuracy: 0.862 - ETA: 1:36:46 - loss: 0.4124 - accuracy: 0.863 - ETA: 1:36:30 - loss: 0.4051 - accuracy: 0.866 - ETA: 1:36:24 - loss: 0.4252 - accuracy: 0.861 - ETA: 1:36:12 - loss: 0.4231 - accuracy: 0.862 - ETA: 1:36:00 - loss: 0.4152 - accuracy: 0.866 - ETA: 1:35:42 - loss: 0.4292 - accuracy: 0.863 - ETA: 1:35:24 - loss: 0.4265 - accuracy: 0.864 - ETA: 1:35:05 - loss: 0.4338 - accuracy: 0.862 - ETA: 1:34:53 - loss: 0.4334 - accuracy: 0.859 - ETA: 1:34:39 - loss: 0.4239 - accuracy: 0.863 - ETA: 1:34:27 - loss: 0.4206 - accuracy: 0.864 - ETA: 1:34:12 - loss: 0.4241 - accuracy: 0.862 - ETA: 1:34:00 - loss: 0.4185 - accuracy: 0.864 - ETA: 1:33:46 - loss: 0.4105 - accuracy: 0.867 - ETA: 1:33:35 - loss: 0.4115 - accuracy: 0.867 - ETA: 1:33:23 - loss: 0.4104 - accuracy: 0.867 - ETA: 1:33:13 - loss: 0.4091 - accuracy: 0.867 - ETA: 1:33:02 - loss: 0.4173 - accuracy: 0.864 - ETA: 1:32:50 - loss: 0.4150 - accuracy: 0.864 - ETA: 1:32:39 - loss: 0.4149 - accuracy: 0.864 - ETA: 1:32:28 - loss: 0.4180 - accuracy: 0.863 - ETA: 1:32:17 - loss: 0.4173 - accuracy: 0.863 - ETA: 1:32:04 - loss: 0.4129 - accuracy: 0.864 - ETA: 1:31:50 - loss: 0.4132 - accuracy: 0.864 - ETA: 1:31:41 - loss: 0.4136 - accuracy: 0.864 - ETA: 1:31:28 - loss: 0.4148 - accuracy: 0.863 - ETA: 1:31:17 - loss: 0.4119 - accuracy: 0.864 - ETA: 1:31:06 - loss: 0.4101 - accuracy: 0.865 - ETA: 1:30:55 - loss: 0.4191 - accuracy: 0.862 - ETA: 1:30:40 - loss: 0.4198 - accuracy: 0.862 - ETA: 1:30:25 - loss: 0.4186 - accuracy: 0.862 - ETA: 1:30:10 - loss: 0.4167 - accuracy: 0.863 - ETA: 1:29:56 - loss: 0.4150 - accuracy: 0.863 - ETA: 1:29:43 - loss: 0.4124 - accuracy: 0.864 - ETA: 1:29:32 - loss: 0.4108 - accuracy: 0.865 - ETA: 1:29:21 - loss: 0.4096 - accuracy: 0.865 - ETA: 1:29:08 - loss: 0.4134 - accuracy: 0.864 - ETA: 1:28:57 - loss: 0.4130 - accuracy: 0.864 - ETA: 1:28:45 - loss: 0.4121 - accuracy: 0.865 - ETA: 1:28:35 - loss: 0.4189 - accuracy: 0.862 - ETA: 1:28:23 - loss: 0.4198 - accuracy: 0.862 - ETA: 1:28:11 - loss: 0.4200 - accuracy: 0.862 - ETA: 1:27:58 - loss: 0.4224 - accuracy: 0.861 - ETA: 1:27:45 - loss: 0.4216 - accuracy: 0.861 - ETA: 1:27:33 - loss: 0.4256 - accuracy: 0.860 - ETA: 1:27:22 - loss: 0.4255 - accuracy: 0.860 - ETA: 1:27:11 - loss: 0.4251 - accuracy: 0.860 - ETA: 1:26:59 - loss: 0.4276 - accuracy: 0.859 - ETA: 1:26:48 - loss: 0.4358 - accuracy: 0.857 - ETA: 1:26:36 - loss: 0.4340 - accuracy: 0.858 - ETA: 1:26:26 - loss: 0.4325 - accuracy: 0.859 - ETA: 1:26:13 - loss: 0.4319 - accuracy: 0.859 - ETA: 1:26:01 - loss: 0.4310 - accuracy: 0.859 - ETA: 1:25:49 - loss: 0.4308 - accuracy: 0.860 - ETA: 1:25:36 - loss: 0.4296 - accuracy: 0.860 - ETA: 1:25:26 - loss: 0.4324 - accuracy: 0.859 - ETA: 1:25:15 - loss: 0.4312 - accuracy: 0.860 - ETA: 1:25:03 - loss: 0.4312 - accuracy: 0.860 - ETA: 1:24:51 - loss: 0.4309 - accuracy: 0.860 - ETA: 1:24:39 - loss: 0.4317 - accuracy: 0.859 - ETA: 1:24:27 - loss: 0.4289 - accuracy: 0.860 - ETA: 1:24:16 - loss: 0.4275 - accuracy: 0.861 - ETA: 1:24:05 - loss: 0.4281 - accuracy: 0.861 - ETA: 1:23:54 - loss: 0.4265 - accuracy: 0.861 - ETA: 1:23:42 - loss: 0.4295 - accuracy: 0.861 - ETA: 1:23:31 - loss: 0.4293 - accuracy: 0.861 - ETA: 1:23:19 - loss: 0.4283 - accuracy: 0.861 - ETA: 1:23:08 - loss: 0.4279 - accuracy: 0.861 - ETA: 1:22:56 - loss: 0.4305 - accuracy: 0.860 - ETA: 1:22:44 - loss: 0.4323 - accuracy: 0.860 - ETA: 1:22:33 - loss: 0.4315 - accuracy: 0.860 - ETA: 1:22:21 - loss: 0.4316 - accuracy: 0.860 - ETA: 1:22:11 - loss: 0.4323 - accuracy: 0.860 - ETA: 1:21:59 - loss: 0.4301 - accuracy: 0.860 - ETA: 1:21:48 - loss: 0.4330 - accuracy: 0.859 - ETA: 1:21:37 - loss: 0.4336 - accuracy: 0.859 - ETA: 1:21:26 - loss: 0.4323 - accuracy: 0.860 - ETA: 1:21:15 - loss: 0.4309 - accuracy: 0.860 - ETA: 1:21:04 - loss: 0.4327 - accuracy: 0.859 - ETA: 1:20:53 - loss: 0.4321 - accuracy: 0.859 - ETA: 1:20:41 - loss: 0.4309 - accuracy: 0.860 - ETA: 1:20:30 - loss: 0.4300 - accuracy: 0.860 - ETA: 1:20:19 - loss: 0.4298 - accuracy: 0.860 - ETA: 1:20:07 - loss: 0.4296 - accuracy: 0.860 - ETA: 1:19:55 - loss: 0.4290 - accuracy: 0.861 - ETA: 1:19:44 - loss: 0.4275 - accuracy: 0.861 - ETA: 1:19:32 - loss: 0.4269 - accuracy: 0.861 - ETA: 1:19:21 - loss: 0.4272 - accuracy: 0.861 - ETA: 1:19:09 - loss: 0.4286 - accuracy: 0.861 - ETA: 1:18:57 - loss: 0.4283 - accuracy: 0.861 - ETA: 1:18:46 - loss: 0.4289 - accuracy: 0.861 - ETA: 1:18:34 - loss: 0.4281 - accuracy: 0.861 - ETA: 1:18:23 - loss: 0.4278 - accuracy: 0.861 - ETA: 1:18:11 - loss: 0.4265 - accuracy: 0.861 - ETA: 1:18:00 - loss: 0.4260 - accuracy: 0.861 - ETA: 1:17:48 - loss: 0.4268 - accuracy: 0.861 - ETA: 1:17:36 - loss: 0.4263 - accuracy: 0.861 - ETA: 1:17:25 - loss: 0.4257 - accuracy: 0.862 - ETA: 1:17:13 - loss: 0.4252 - accuracy: 0.862 - ETA: 1:17:01 - loss: 0.4260 - accuracy: 0.862 - ETA: 1:16:50 - loss: 0.4270 - accuracy: 0.861 - ETA: 1:16:38 - loss: 0.4264 - accuracy: 0.861 - ETA: 1:16:27 - loss: 0.4263 - accuracy: 0.861 - ETA: 1:16:15 - loss: 0.4252 - accuracy: 0.862 - ETA: 1:16:03 - loss: 0.4259 - accuracy: 0.861 - ETA: 1:15:52 - loss: 0.4252 - accuracy: 0.862 - ETA: 1:15:40 - loss: 0.4245 - accuracy: 0.862 - ETA: 1:15:29 - loss: 0.4264 - accuracy: 0.861 - ETA: 1:15:18 - loss: 0.4272 - accuracy: 0.861 - ETA: 1:15:07 - loss: 0.4261 - accuracy: 0.861 - ETA: 1:14:56 - loss: 0.4280 - accuracy: 0.860 - ETA: 1:14:44 - loss: 0.4279 - accuracy: 0.860 - ETA: 1:14:33 - loss: 0.4269 - accuracy: 0.861 - ETA: 1:14:22 - loss: 0.4266 - accuracy: 0.861 - ETA: 1:14:11 - loss: 0.4273 - accuracy: 0.861 - ETA: 1:13:59 - loss: 0.4271 - accuracy: 0.861 - ETA: 1:13:48 - loss: 0.4278 - accuracy: 0.861 - ETA: 1:13:36 - loss: 0.4280 - accuracy: 0.861 - ETA: 1:13:25 - loss: 0.4278 - accuracy: 0.861 - ETA: 1:13:13 - loss: 0.4275 - accuracy: 0.861 - ETA: 1:13:02 - loss: 0.4270 - accuracy: 0.861 - ETA: 1:12:50 - loss: 0.4266 - accuracy: 0.861 - ETA: 1:12:38 - loss: 0.4273 - accuracy: 0.861 - ETA: 1:12:26 - loss: 0.4277 - accuracy: 0.860 - ETA: 1:12:14 - loss: 0.4293 - accuracy: 0.860 - ETA: 1:12:02 - loss: 0.4297 - accuracy: 0.860 - ETA: 1:11:51 - loss: 0.4289 - accuracy: 0.860 - ETA: 1:11:39 - loss: 0.4290 - accuracy: 0.860 - ETA: 1:11:28 - loss: 0.4306 - accuracy: 0.860 - ETA: 1:11:16 - loss: 0.4299 - accuracy: 0.860 - ETA: 1:11:04 - loss: 0.4292 - accuracy: 0.860 - ETA: 1:10:52 - loss: 0.4283 - accuracy: 0.861 - ETA: 1:10:41 - loss: 0.4330 - accuracy: 0.859 - ETA: 1:10:29 - loss: 0.4322 - accuracy: 0.859 - ETA: 1:10:18 - loss: 0.4323 - accuracy: 0.859 - ETA: 1:10:06 - loss: 0.4326 - accuracy: 0.859 - ETA: 1:09:55 - loss: 0.4338 - accuracy: 0.859 - ETA: 1:09:43 - loss: 0.4339 - accuracy: 0.859 - ETA: 1:09:32 - loss: 0.4346 - accuracy: 0.859 - ETA: 1:09:21 - loss: 0.4366 - accuracy: 0.858 - ETA: 1:09:09 - loss: 0.4359 - accuracy: 0.858 - ETA: 1:08:58 - loss: 0.4351 - accuracy: 0.859 - ETA: 1:08:47 - loss: 0.4346 - accuracy: 0.859 - ETA: 1:08:36 - loss: 0.4338 - accuracy: 0.859 - ETA: 1:08:24 - loss: 0.4348 - accuracy: 0.859 - ETA: 1:08:12 - loss: 0.4350 - accuracy: 0.859 - ETA: 1:08:00 - loss: 0.4343 - accuracy: 0.859 - ETA: 1:07:49 - loss: 0.4341 - accuracy: 0.859 - ETA: 1:07:38 - loss: 0.4338 - accuracy: 0.859 - ETA: 1:07:26 - loss: 0.4331 - accuracy: 0.859 - ETA: 1:07:15 - loss: 0.4333 - accuracy: 0.859 - ETA: 1:07:03 - loss: 0.4336 - accuracy: 0.859 - ETA: 1:06:51 - loss: 0.4329 - accuracy: 0.860 - ETA: 1:06:40 - loss: 0.4324 - accuracy: 0.860 - ETA: 1:06:28 - loss: 0.4313 - accuracy: 0.860 - ETA: 1:06:17 - loss: 0.4317 - accuracy: 0.860 - ETA: 1:06:05 - loss: 0.4332 - accuracy: 0.860 - ETA: 1:05:54 - loss: 0.4336 - accuracy: 0.859 - ETA: 1:05:42 - loss: 0.4335 - accuracy: 0.859 - ETA: 1:05:31 - loss: 0.4348 - accuracy: 0.859 - ETA: 1:05:20 - loss: 0.4366 - accuracy: 0.8589346/512 [===================>..........] - ETA: 1:05:08 - loss: 0.4361 - accuracy: 0.859 - ETA: 1:04:57 - loss: 0.4363 - accuracy: 0.858 - ETA: 1:04:45 - loss: 0.4372 - accuracy: 0.858 - ETA: 1:04:33 - loss: 0.4367 - accuracy: 0.858 - ETA: 1:04:22 - loss: 0.4363 - accuracy: 0.859 - ETA: 1:04:10 - loss: 0.4360 - accuracy: 0.859 - ETA: 1:03:59 - loss: 0.4365 - accuracy: 0.858 - ETA: 1:03:48 - loss: 0.4367 - accuracy: 0.858 - ETA: 1:03:36 - loss: 0.4363 - accuracy: 0.858 - ETA: 1:03:25 - loss: 0.4365 - accuracy: 0.858 - ETA: 1:03:13 - loss: 0.4361 - accuracy: 0.858 - ETA: 1:03:01 - loss: 0.4354 - accuracy: 0.859 - ETA: 1:02:49 - loss: 0.4348 - accuracy: 0.859 - ETA: 1:02:38 - loss: 0.4340 - accuracy: 0.859 - ETA: 1:02:26 - loss: 0.4352 - accuracy: 0.859 - ETA: 1:02:15 - loss: 0.4345 - accuracy: 0.859 - ETA: 1:02:03 - loss: 0.4342 - accuracy: 0.859 - ETA: 1:01:52 - loss: 0.4343 - accuracy: 0.859 - ETA: 1:01:40 - loss: 0.4340 - accuracy: 0.859 - ETA: 1:01:29 - loss: 0.4351 - accuracy: 0.859 - ETA: 1:01:18 - loss: 0.4348 - accuracy: 0.859 - ETA: 1:01:07 - loss: 0.4343 - accuracy: 0.859 - ETA: 1:00:55 - loss: 0.4344 - accuracy: 0.859 - ETA: 1:00:44 - loss: 0.4336 - accuracy: 0.859 - ETA: 1:00:33 - loss: 0.4337 - accuracy: 0.859 - ETA: 1:00:22 - loss: 0.4339 - accuracy: 0.859 - ETA: 1:00:11 - loss: 0.4333 - accuracy: 0.859 - ETA: 59:59 - loss: 0.4328 - accuracy: 0.8600  - ETA: 59:48 - loss: 0.4337 - accuracy: 0.859 - ETA: 59:37 - loss: 0.4334 - accuracy: 0.859 - ETA: 59:25 - loss: 0.4341 - accuracy: 0.859 - ETA: 59:14 - loss: 0.4332 - accuracy: 0.859 - ETA: 59:03 - loss: 0.4326 - accuracy: 0.860 - ETA: 58:52 - loss: 0.4324 - accuracy: 0.860 - ETA: 58:40 - loss: 0.4319 - accuracy: 0.860 - ETA: 58:29 - loss: 0.4311 - accuracy: 0.860 - ETA: 58:17 - loss: 0.4304 - accuracy: 0.860 - ETA: 58:06 - loss: 0.4308 - accuracy: 0.860 - ETA: 57:55 - loss: 0.4302 - accuracy: 0.860 - ETA: 57:43 - loss: 0.4306 - accuracy: 0.860 - ETA: 57:32 - loss: 0.4304 - accuracy: 0.860 - ETA: 57:21 - loss: 0.4303 - accuracy: 0.860 - ETA: 57:10 - loss: 0.4311 - accuracy: 0.860 - ETA: 56:58 - loss: 0.4311 - accuracy: 0.860 - ETA: 56:47 - loss: 0.4307 - accuracy: 0.860 - ETA: 56:35 - loss: 0.4299 - accuracy: 0.860 - ETA: 56:23 - loss: 0.4293 - accuracy: 0.861 - ETA: 56:12 - loss: 0.4294 - accuracy: 0.861 - ETA: 56:00 - loss: 0.4296 - accuracy: 0.861 - ETA: 55:48 - loss: 0.4295 - accuracy: 0.861 - ETA: 55:37 - loss: 0.4297 - accuracy: 0.861 - ETA: 55:26 - loss: 0.4295 - accuracy: 0.861 - ETA: 55:14 - loss: 0.4294 - accuracy: 0.861 - ETA: 55:03 - loss: 0.4292 - accuracy: 0.861 - ETA: 54:51 - loss: 0.4290 - accuracy: 0.861 - ETA: 54:40 - loss: 0.4284 - accuracy: 0.861 - ETA: 54:28 - loss: 0.4280 - accuracy: 0.861 - ETA: 54:16 - loss: 0.4276 - accuracy: 0.861 - ETA: 54:05 - loss: 0.4283 - accuracy: 0.861 - ETA: 53:54 - loss: 0.4284 - accuracy: 0.861 - ETA: 53:42 - loss: 0.4285 - accuracy: 0.861 - ETA: 53:31 - loss: 0.4295 - accuracy: 0.861 - ETA: 53:19 - loss: 0.4298 - accuracy: 0.861 - ETA: 53:08 - loss: 0.4297 - accuracy: 0.861 - ETA: 52:56 - loss: 0.4293 - accuracy: 0.861 - ETA: 52:44 - loss: 0.4303 - accuracy: 0.860 - ETA: 52:32 - loss: 0.4303 - accuracy: 0.860 - ETA: 52:21 - loss: 0.4307 - accuracy: 0.860 - ETA: 52:09 - loss: 0.4301 - accuracy: 0.860 - ETA: 51:57 - loss: 0.4300 - accuracy: 0.860 - ETA: 51:45 - loss: 0.4300 - accuracy: 0.860 - ETA: 51:33 - loss: 0.4302 - accuracy: 0.860 - ETA: 51:22 - loss: 0.4297 - accuracy: 0.860 - ETA: 51:10 - loss: 0.4295 - accuracy: 0.861 - ETA: 50:58 - loss: 0.4292 - accuracy: 0.861 - ETA: 50:46 - loss: 0.4292 - accuracy: 0.861 - ETA: 50:34 - loss: 0.4285 - accuracy: 0.861 - ETA: 50:23 - loss: 0.4291 - accuracy: 0.861 - ETA: 50:11 - loss: 0.4285 - accuracy: 0.861 - ETA: 49:59 - loss: 0.4283 - accuracy: 0.861 - ETA: 49:47 - loss: 0.4281 - accuracy: 0.861 - ETA: 49:36 - loss: 0.4280 - accuracy: 0.861 - ETA: 49:24 - loss: 0.4282 - accuracy: 0.861 - ETA: 49:12 - loss: 0.4274 - accuracy: 0.861 - ETA: 49:00 - loss: 0.4284 - accuracy: 0.861 - ETA: 48:49 - loss: 0.4284 - accuracy: 0.861 - ETA: 48:37 - loss: 0.4285 - accuracy: 0.861 - ETA: 48:25 - loss: 0.4281 - accuracy: 0.861 - ETA: 48:14 - loss: 0.4284 - accuracy: 0.861 - ETA: 48:02 - loss: 0.4278 - accuracy: 0.861 - ETA: 47:50 - loss: 0.4278 - accuracy: 0.861 - ETA: 47:38 - loss: 0.4275 - accuracy: 0.861 - ETA: 47:27 - loss: 0.4270 - accuracy: 0.862 - ETA: 47:15 - loss: 0.4271 - accuracy: 0.862 - ETA: 47:03 - loss: 0.4270 - accuracy: 0.862 - ETA: 46:52 - loss: 0.4263 - accuracy: 0.862 - ETA: 46:40 - loss: 0.4273 - accuracy: 0.862 - ETA: 46:28 - loss: 0.4272 - accuracy: 0.862 - ETA: 46:17 - loss: 0.4270 - accuracy: 0.862 - ETA: 46:05 - loss: 0.4266 - accuracy: 0.862 - ETA: 45:53 - loss: 0.4265 - accuracy: 0.862 - ETA: 45:42 - loss: 0.4272 - accuracy: 0.861 - ETA: 45:30 - loss: 0.4267 - accuracy: 0.862 - ETA: 45:18 - loss: 0.4261 - accuracy: 0.862 - ETA: 45:07 - loss: 0.4259 - accuracy: 0.862 - ETA: 44:55 - loss: 0.4255 - accuracy: 0.862 - ETA: 44:43 - loss: 0.4258 - accuracy: 0.862 - ETA: 44:32 - loss: 0.4255 - accuracy: 0.862 - ETA: 44:20 - loss: 0.4250 - accuracy: 0.862 - ETA: 44:08 - loss: 0.4253 - accuracy: 0.862 - ETA: 43:57 - loss: 0.4251 - accuracy: 0.862 - ETA: 43:45 - loss: 0.4254 - accuracy: 0.862 - ETA: 43:34 - loss: 0.4256 - accuracy: 0.862 - ETA: 43:22 - loss: 0.4253 - accuracy: 0.862 - ETA: 43:10 - loss: 0.4252 - accuracy: 0.862 - ETA: 42:59 - loss: 0.4250 - accuracy: 0.862 - ETA: 42:47 - loss: 0.4247 - accuracy: 0.862 - ETA: 42:36 - loss: 0.4246 - accuracy: 0.862 - ETA: 42:24 - loss: 0.4249 - accuracy: 0.862 - ETA: 42:12 - loss: 0.4245 - accuracy: 0.862 - ETA: 42:01 - loss: 0.4241 - accuracy: 0.862 - ETA: 41:49 - loss: 0.4241 - accuracy: 0.862 - ETA: 41:37 - loss: 0.4244 - accuracy: 0.862 - ETA: 41:26 - loss: 0.4241 - accuracy: 0.862 - ETA: 41:14 - loss: 0.4239 - accuracy: 0.862 - ETA: 41:03 - loss: 0.4236 - accuracy: 0.863 - ETA: 40:51 - loss: 0.4239 - accuracy: 0.862 - ETA: 40:39 - loss: 0.4236 - accuracy: 0.863 - ETA: 40:28 - loss: 0.4234 - accuracy: 0.863 - ETA: 40:16 - loss: 0.4239 - accuracy: 0.862 - ETA: 40:05 - loss: 0.4237 - accuracy: 0.863 - ETA: 39:53 - loss: 0.4237 - accuracy: 0.863 - ETA: 39:41 - loss: 0.4237 - accuracy: 0.863 - ETA: 39:30 - loss: 0.4238 - accuracy: 0.862 - ETA: 39:18 - loss: 0.4237 - accuracy: 0.863 - ETA: 39:07 - loss: 0.4233 - accuracy: 0.863 - ETA: 38:55 - loss: 0.4237 - accuracy: 0.863 - ETA: 38:43 - loss: 0.4238 - accuracy: 0.862 - ETA: 38:32 - loss: 0.4245 - accuracy: 0.862 - ETA: 38:20 - loss: 0.4241 - accuracy: 0.862 - ETA: 38:09 - loss: 0.4240 - accuracy: 0.862 - ETA: 37:57 - loss: 0.4239 - accuracy: 0.862 - ETA: 37:45 - loss: 0.4235 - accuracy: 0.863 - ETA: 37:34 - loss: 0.4233 - accuracy: 0.863 - ETA: 37:22 - loss: 0.4237 - accuracy: 0.862 - ETA: 37:11 - loss: 0.4242 - accuracy: 0.862 - ETA: 36:59 - loss: 0.4240 - accuracy: 0.862 - ETA: 36:48 - loss: 0.4244 - accuracy: 0.862 - ETA: 36:36 - loss: 0.4244 - accuracy: 0.862 - ETA: 36:25 - loss: 0.4241 - accuracy: 0.862 - ETA: 36:13 - loss: 0.4249 - accuracy: 0.862 - ETA: 36:01 - loss: 0.4247 - accuracy: 0.862 - ETA: 35:50 - loss: 0.4245 - accuracy: 0.862 - ETA: 35:38 - loss: 0.4242 - accuracy: 0.862 - ETA: 35:27 - loss: 0.4244 - accuracy: 0.862 - ETA: 35:15 - loss: 0.4248 - accuracy: 0.862 - ETA: 35:04 - loss: 0.4247 - accuracy: 0.862 - ETA: 34:52 - loss: 0.4246 - accuracy: 0.862 - ETA: 34:41 - loss: 0.4254 - accuracy: 0.862 - ETA: 34:29 - loss: 0.4252 - accuracy: 0.862 - ETA: 34:17 - loss: 0.4256 - accuracy: 0.862 - ETA: 34:06 - loss: 0.4259 - accuracy: 0.862 - ETA: 33:54 - loss: 0.4264 - accuracy: 0.861 - ETA: 33:43 - loss: 0.4261 - accuracy: 0.862 - ETA: 33:31 - loss: 0.4260 - accuracy: 0.862 - ETA: 33:20 - loss: 0.4258 - accuracy: 0.862 - ETA: 33:08 - loss: 0.4259 - accuracy: 0.862 - ETA: 32:57 - loss: 0.4259 - accuracy: 0.862 - ETA: 32:45 - loss: 0.4260 - accuracy: 0.862 - ETA: 32:34 - loss: 0.4258 - accuracy: 0.862 - ETA: 32:22 - loss: 0.4254 - accuracy: 0.862 - ETA: 32:11 - loss: 0.4251 - accuracy: 0.862 - ETA: 31:59 - loss: 0.4246 - accuracy: 0.862 - ETA: 31:48 - loss: 0.4255 - accuracy: 0.862 - ETA: 31:36 - loss: 0.4254 - accuracy: 0.862 - ETA: 31:25 - loss: 0.4248 - accuracy: 0.8626"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - ETA: 31:13 - loss: 0.4245 - accuracy: 0.862 - ETA: 31:02 - loss: 0.4241 - accuracy: 0.862 - ETA: 30:50 - loss: 0.4247 - accuracy: 0.862 - ETA: 30:39 - loss: 0.4246 - accuracy: 0.862 - ETA: 30:27 - loss: 0.4247 - accuracy: 0.862 - ETA: 30:16 - loss: 0.4246 - accuracy: 0.862 - ETA: 30:04 - loss: 0.4254 - accuracy: 0.862 - ETA: 29:53 - loss: 0.4260 - accuracy: 0.862 - ETA: 29:41 - loss: 0.4257 - accuracy: 0.862 - ETA: 29:30 - loss: 0.4257 - accuracy: 0.862 - ETA: 29:18 - loss: 0.4263 - accuracy: 0.862 - ETA: 29:07 - loss: 0.4261 - accuracy: 0.862 - ETA: 28:55 - loss: 0.4258 - accuracy: 0.862 - ETA: 28:44 - loss: 0.4256 - accuracy: 0.862 - ETA: 28:32 - loss: 0.4257 - accuracy: 0.862 - ETA: 28:21 - loss: 0.4256 - accuracy: 0.862 - ETA: 28:10 - loss: 0.4252 - accuracy: 0.862 - ETA: 27:58 - loss: 0.4254 - accuracy: 0.862 - ETA: 27:47 - loss: 0.4252 - accuracy: 0.862 - ETA: 27:35 - loss: 0.4248 - accuracy: 0.862 - ETA: 27:24 - loss: 0.4246 - accuracy: 0.862 - ETA: 27:12 - loss: 0.4245 - accuracy: 0.862 - ETA: 27:01 - loss: 0.4245 - accuracy: 0.862 - ETA: 26:49 - loss: 0.4244 - accuracy: 0.862 - ETA: 26:38 - loss: 0.4241 - accuracy: 0.862 - ETA: 26:26 - loss: 0.4241 - accuracy: 0.862 - ETA: 26:15 - loss: 0.4238 - accuracy: 0.862 - ETA: 26:04 - loss: 0.4238 - accuracy: 0.862 - ETA: 25:52 - loss: 0.4238 - accuracy: 0.862 - ETA: 25:41 - loss: 0.4236 - accuracy: 0.862 - ETA: 25:29 - loss: 0.4233 - accuracy: 0.862 - ETA: 25:18 - loss: 0.4230 - accuracy: 0.863 - ETA: 25:06 - loss: 0.4235 - accuracy: 0.862 - ETA: 24:55 - loss: 0.4234 - accuracy: 0.862 - ETA: 24:44 - loss: 0.4230 - accuracy: 0.863 - ETA: 24:32 - loss: 0.4235 - accuracy: 0.863 - ETA: 24:21 - loss: 0.4234 - accuracy: 0.863 - ETA: 24:09 - loss: 0.4236 - accuracy: 0.863 - ETA: 23:58 - loss: 0.4234 - accuracy: 0.862 - ETA: 23:46 - loss: 0.4229 - accuracy: 0.863 - ETA: 23:35 - loss: 0.4227 - accuracy: 0.863 - ETA: 23:24 - loss: 0.4228 - accuracy: 0.863 - ETA: 23:12 - loss: 0.4224 - accuracy: 0.863 - ETA: 23:01 - loss: 0.4219 - accuracy: 0.863 - ETA: 22:49 - loss: 0.4219 - accuracy: 0.863 - ETA: 22:38 - loss: 0.4217 - accuracy: 0.863 - ETA: 22:27 - loss: 0.4215 - accuracy: 0.863 - ETA: 22:15 - loss: 0.4219 - accuracy: 0.863 - ETA: 22:04 - loss: 0.4217 - accuracy: 0.863 - ETA: 21:52 - loss: 0.4216 - accuracy: 0.863 - ETA: 21:41 - loss: 0.4217 - accuracy: 0.863 - ETA: 21:30 - loss: 0.4216 - accuracy: 0.863 - ETA: 21:18 - loss: 0.4212 - accuracy: 0.863 - ETA: 21:07 - loss: 0.4211 - accuracy: 0.863 - ETA: 20:55 - loss: 0.4211 - accuracy: 0.863 - ETA: 20:44 - loss: 0.4211 - accuracy: 0.863 - ETA: 20:33 - loss: 0.4208 - accuracy: 0.863 - ETA: 20:21 - loss: 0.4206 - accuracy: 0.863 - ETA: 20:10 - loss: 0.4213 - accuracy: 0.863 - ETA: 19:59 - loss: 0.4213 - accuracy: 0.863 - ETA: 19:47 - loss: 0.4212 - accuracy: 0.863 - ETA: 19:36 - loss: 0.4209 - accuracy: 0.863 - ETA: 19:24 - loss: 0.4207 - accuracy: 0.863 - ETA: 19:13 - loss: 0.4203 - accuracy: 0.863 - ETA: 19:02 - loss: 0.4201 - accuracy: 0.863 - ETA: 18:50 - loss: 0.4199 - accuracy: 0.863 - ETA: 18:39 - loss: 0.4202 - accuracy: 0.863 - ETA: 18:28 - loss: 0.4201 - accuracy: 0.863 - ETA: 18:16 - loss: 0.4199 - accuracy: 0.863 - ETA: 18:05 - loss: 0.4205 - accuracy: 0.863 - ETA: 17:53 - loss: 0.4206 - accuracy: 0.863 - ETA: 17:42 - loss: 0.4205 - accuracy: 0.863 - ETA: 17:31 - loss: 0.4207 - accuracy: 0.863 - ETA: 17:19 - loss: 0.4205 - accuracy: 0.863 - ETA: 17:08 - loss: 0.4210 - accuracy: 0.863 - ETA: 16:57 - loss: 0.4210 - accuracy: 0.863 - ETA: 16:45 - loss: 0.4208 - accuracy: 0.863 - ETA: 16:34 - loss: 0.4211 - accuracy: 0.863 - ETA: 16:23 - loss: 0.4221 - accuracy: 0.863 - ETA: 16:11 - loss: 0.4218 - accuracy: 0.863 - ETA: 16:00 - loss: 0.4216 - accuracy: 0.863 - ETA: 15:48 - loss: 0.4215 - accuracy: 0.863 - ETA: 15:37 - loss: 0.4214 - accuracy: 0.863 - ETA: 15:26 - loss: 0.4213 - accuracy: 0.863 - ETA: 15:14 - loss: 0.4211 - accuracy: 0.863 - ETA: 15:03 - loss: 0.4215 - accuracy: 0.863 - ETA: 14:52 - loss: 0.4213 - accuracy: 0.863 - ETA: 14:41 - loss: 0.4212 - accuracy: 0.863 - ETA: 14:29 - loss: 0.4212 - accuracy: 0.863 - ETA: 14:18 - loss: 0.4212 - accuracy: 0.863 - ETA: 14:07 - loss: 0.4208 - accuracy: 0.863 - ETA: 13:55 - loss: 0.4206 - accuracy: 0.863 - ETA: 13:44 - loss: 0.4206 - accuracy: 0.863 - ETA: 13:33 - loss: 0.4203 - accuracy: 0.863 - ETA: 13:21 - loss: 0.4208 - accuracy: 0.863 - ETA: 13:10 - loss: 0.4207 - accuracy: 0.863 - ETA: 12:59 - loss: 0.4205 - accuracy: 0.863 - ETA: 12:47 - loss: 0.4204 - accuracy: 0.863 - ETA: 12:36 - loss: 0.4208 - accuracy: 0.863 - ETA: 12:25 - loss: 0.4211 - accuracy: 0.863 - ETA: 12:13 - loss: 0.4210 - accuracy: 0.863 - ETA: 12:02 - loss: 0.4209 - accuracy: 0.863 - ETA: 11:51 - loss: 0.4210 - accuracy: 0.863 - ETA: 11:39 - loss: 0.4206 - accuracy: 0.863 - ETA: 11:28 - loss: 0.4211 - accuracy: 0.863 - ETA: 11:17 - loss: 0.4212 - accuracy: 0.863 - ETA: 11:06 - loss: 0.4209 - accuracy: 0.863 - ETA: 10:54 - loss: 0.4206 - accuracy: 0.863 - ETA: 10:43 - loss: 0.4208 - accuracy: 0.863 - ETA: 10:32 - loss: 0.4207 - accuracy: 0.863 - ETA: 10:20 - loss: 0.4204 - accuracy: 0.863 - ETA: 10:09 - loss: 0.4203 - accuracy: 0.863 - ETA: 9:58 - loss: 0.4202 - accuracy: 0.863 - ETA: 9:47 - loss: 0.4201 - accuracy: 0.86 - ETA: 9:35 - loss: 0.4200 - accuracy: 0.86 - ETA: 9:24 - loss: 0.4197 - accuracy: 0.86 - ETA: 9:13 - loss: 0.4195 - accuracy: 0.86 - ETA: 9:02 - loss: 0.4196 - accuracy: 0.86 - ETA: 8:50 - loss: 0.4198 - accuracy: 0.86 - ETA: 8:39 - loss: 0.4197 - accuracy: 0.86 - ETA: 8:28 - loss: 0.4198 - accuracy: 0.86 - ETA: 8:17 - loss: 0.4196 - accuracy: 0.86 - ETA: 8:05 - loss: 0.4195 - accuracy: 0.86 - ETA: 7:54 - loss: 0.4192 - accuracy: 0.86 - ETA: 7:43 - loss: 0.4191 - accuracy: 0.86 - ETA: 7:31 - loss: 0.4192 - accuracy: 0.86 - ETA: 7:20 - loss: 0.4191 - accuracy: 0.86 - ETA: 7:09 - loss: 0.4189 - accuracy: 0.86 - ETA: 6:58 - loss: 0.4188 - accuracy: 0.86 - ETA: 6:46 - loss: 0.4189 - accuracy: 0.86 - ETA: 6:35 - loss: 0.4191 - accuracy: 0.86 - ETA: 6:24 - loss: 0.4189 - accuracy: 0.86 - ETA: 6:12 - loss: 0.4189 - accuracy: 0.86 - ETA: 6:01 - loss: 0.4186 - accuracy: 0.86 - ETA: 5:50 - loss: 0.4187 - accuracy: 0.86 - ETA: 5:38 - loss: 0.4185 - accuracy: 0.86 - ETA: 5:27 - loss: 0.4183 - accuracy: 0.86 - ETA: 5:16 - loss: 0.4187 - accuracy: 0.86 - ETA: 5:05 - loss: 0.4189 - accuracy: 0.86 - ETA: 4:53 - loss: 0.4187 - accuracy: 0.86 - ETA: 4:42 - loss: 0.4191 - accuracy: 0.86 - ETA: 4:31 - loss: 0.4190 - accuracy: 0.86 - ETA: 4:19 - loss: 0.4187 - accuracy: 0.86 - ETA: 4:08 - loss: 0.4186 - accuracy: 0.86 - ETA: 3:57 - loss: 0.4188 - accuracy: 0.86 - ETA: 3:45 - loss: 0.4187 - accuracy: 0.86 - ETA: 3:34 - loss: 0.4188 - accuracy: 0.86 - ETA: 3:23 - loss: 0.4188 - accuracy: 0.86 - ETA: 3:12 - loss: 0.4188 - accuracy: 0.86 - ETA: 3:00 - loss: 0.4187 - accuracy: 0.86 - ETA: 2:49 - loss: 0.4185 - accuracy: 0.86 - ETA: 2:38 - loss: 0.4184 - accuracy: 0.86 - ETA: 2:26 - loss: 0.4185 - accuracy: 0.86 - ETA: 2:15 - loss: 0.4185 - accuracy: 0.86 - ETA: 2:04 - loss: 0.4189 - accuracy: 0.86 - ETA: 1:52 - loss: 0.4190 - accuracy: 0.86 - ETA: 1:41 - loss: 0.4187 - accuracy: 0.86 - ETA: 1:30 - loss: 0.4187 - accuracy: 0.86 - ETA: 1:19 - loss: 0.4191 - accuracy: 0.86 - ETA: 1:07 - loss: 0.4189 - accuracy: 0.86 - ETA: 56s - loss: 0.4187 - accuracy: 0.8642 - ETA: 45s - loss: 0.4184 - accuracy: 0.864 - ETA: 33s - loss: 0.4196 - accuracy: 0.864 - ETA: 22s - loss: 0.4194 - accuracy: 0.864 - ETA: 11s - loss: 0.4194 - accuracy: 0.864 - 5783s 11s/step - loss: 0.4195 - accuracy: 0.8640\n",
      "Finished Epoch 1\n",
      "Starting Epoch  2\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170/512 [========>.....................] - ETA: 1:45:59 - loss: 0.3640 - accuracy: 0.887 - ETA: 1:41:29 - loss: 0.3523 - accuracy: 0.893 - ETA: 1:40:10 - loss: 0.3387 - accuracy: 0.894 - ETA: 1:38:40 - loss: 0.3648 - accuracy: 0.885 - ETA: 1:37:43 - loss: 0.3825 - accuracy: 0.880 - ETA: 1:36:55 - loss: 0.3718 - accuracy: 0.884 - ETA: 1:36:08 - loss: 0.3669 - accuracy: 0.886 - ETA: 1:35:33 - loss: 0.3511 - accuracy: 0.891 - ETA: 1:35:05 - loss: 0.3650 - accuracy: 0.885 - ETA: 1:34:46 - loss: 0.3928 - accuracy: 0.875 - ETA: 1:34:27 - loss: 0.3999 - accuracy: 0.873 - ETA: 1:34:08 - loss: 0.3990 - accuracy: 0.873 - ETA: 1:33:48 - loss: 0.4173 - accuracy: 0.867 - ETA: 1:33:35 - loss: 0.4378 - accuracy: 0.860 - ETA: 1:33:18 - loss: 0.4316 - accuracy: 0.861 - ETA: 1:32:59 - loss: 0.4317 - accuracy: 0.860 - ETA: 1:32:47 - loss: 0.4384 - accuracy: 0.858 - ETA: 1:32:33 - loss: 0.4328 - accuracy: 0.860 - ETA: 1:32:18 - loss: 0.4284 - accuracy: 0.862 - ETA: 1:32:06 - loss: 0.4255 - accuracy: 0.863 - ETA: 1:31:54 - loss: 0.4289 - accuracy: 0.861 - ETA: 1:31:40 - loss: 0.4306 - accuracy: 0.861 - ETA: 1:31:29 - loss: 0.4270 - accuracy: 0.862 - ETA: 1:31:18 - loss: 0.4275 - accuracy: 0.861 - ETA: 1:31:04 - loss: 0.4247 - accuracy: 0.863 - ETA: 1:30:55 - loss: 0.4197 - accuracy: 0.864 - ETA: 1:30:44 - loss: 0.4160 - accuracy: 0.865 - ETA: 1:30:32 - loss: 0.4109 - accuracy: 0.867 - ETA: 1:30:18 - loss: 0.4181 - accuracy: 0.864 - ETA: 1:30:05 - loss: 0.4138 - accuracy: 0.865 - ETA: 1:29:54 - loss: 0.4116 - accuracy: 0.866 - ETA: 1:29:42 - loss: 0.4114 - accuracy: 0.866 - ETA: 1:29:30 - loss: 0.4100 - accuracy: 0.867 - ETA: 1:29:18 - loss: 0.4152 - accuracy: 0.866 - ETA: 1:29:06 - loss: 0.4135 - accuracy: 0.866 - ETA: 1:28:54 - loss: 0.4108 - accuracy: 0.867 - ETA: 1:28:43 - loss: 0.4114 - accuracy: 0.867 - ETA: 1:28:32 - loss: 0.4077 - accuracy: 0.868 - ETA: 1:28:20 - loss: 0.4079 - accuracy: 0.867 - ETA: 1:28:07 - loss: 0.4091 - accuracy: 0.867 - ETA: 1:27:55 - loss: 0.4063 - accuracy: 0.868 - ETA: 1:27:43 - loss: 0.4043 - accuracy: 0.869 - ETA: 1:27:32 - loss: 0.4088 - accuracy: 0.868 - ETA: 1:27:20 - loss: 0.4076 - accuracy: 0.868 - ETA: 1:27:08 - loss: 0.4096 - accuracy: 0.866 - ETA: 1:26:57 - loss: 0.4059 - accuracy: 0.867 - ETA: 1:26:44 - loss: 0.4036 - accuracy: 0.868 - ETA: 1:26:33 - loss: 0.4031 - accuracy: 0.868 - ETA: 1:26:21 - loss: 0.4015 - accuracy: 0.869 - ETA: 1:26:10 - loss: 0.3985 - accuracy: 0.870 - ETA: 1:25:57 - loss: 0.3961 - accuracy: 0.871 - ETA: 1:25:46 - loss: 0.3975 - accuracy: 0.870 - ETA: 1:25:34 - loss: 0.3954 - accuracy: 0.871 - ETA: 1:25:21 - loss: 0.3971 - accuracy: 0.870 - ETA: 1:25:09 - loss: 0.3967 - accuracy: 0.870 - ETA: 1:24:58 - loss: 0.3965 - accuracy: 0.870 - ETA: 1:24:46 - loss: 0.3997 - accuracy: 0.869 - ETA: 1:24:34 - loss: 0.4000 - accuracy: 0.869 - ETA: 1:24:22 - loss: 0.3986 - accuracy: 0.870 - ETA: 1:24:11 - loss: 0.3964 - accuracy: 0.871 - ETA: 1:24:00 - loss: 0.3943 - accuracy: 0.871 - ETA: 1:23:49 - loss: 0.3950 - accuracy: 0.871 - ETA: 1:23:38 - loss: 0.3959 - accuracy: 0.871 - ETA: 1:23:28 - loss: 0.3958 - accuracy: 0.871 - ETA: 1:23:17 - loss: 0.3968 - accuracy: 0.871 - ETA: 1:23:07 - loss: 0.3964 - accuracy: 0.871 - ETA: 1:22:57 - loss: 0.3964 - accuracy: 0.870 - ETA: 1:22:45 - loss: 0.3961 - accuracy: 0.870 - ETA: 1:22:34 - loss: 0.3956 - accuracy: 0.871 - ETA: 1:22:24 - loss: 0.3940 - accuracy: 0.871 - ETA: 1:22:16 - loss: 0.3931 - accuracy: 0.872 - ETA: 1:22:05 - loss: 0.3920 - accuracy: 0.872 - ETA: 1:21:54 - loss: 0.3944 - accuracy: 0.871 - ETA: 1:21:42 - loss: 0.3949 - accuracy: 0.871 - ETA: 1:21:31 - loss: 0.3954 - accuracy: 0.871 - ETA: 1:21:20 - loss: 0.3987 - accuracy: 0.870 - ETA: 1:21:09 - loss: 0.3993 - accuracy: 0.870 - ETA: 1:21:00 - loss: 0.3991 - accuracy: 0.870 - ETA: 1:20:50 - loss: 0.3983 - accuracy: 0.870 - ETA: 1:20:38 - loss: 0.4012 - accuracy: 0.869 - ETA: 1:20:27 - loss: 0.4012 - accuracy: 0.869 - ETA: 1:20:17 - loss: 0.4028 - accuracy: 0.869 - ETA: 1:20:06 - loss: 0.4012 - accuracy: 0.869 - ETA: 1:19:55 - loss: 0.4010 - accuracy: 0.869 - ETA: 1:19:45 - loss: 0.4011 - accuracy: 0.869 - ETA: 1:19:34 - loss: 0.4018 - accuracy: 0.869 - ETA: 1:19:22 - loss: 0.4004 - accuracy: 0.869 - ETA: 1:19:10 - loss: 0.4000 - accuracy: 0.870 - ETA: 1:18:59 - loss: 0.3995 - accuracy: 0.870 - ETA: 1:18:47 - loss: 0.3996 - accuracy: 0.870 - ETA: 1:18:35 - loss: 0.3981 - accuracy: 0.870 - ETA: 1:18:23 - loss: 0.3997 - accuracy: 0.870 - ETA: 1:18:12 - loss: 0.3981 - accuracy: 0.870 - ETA: 1:18:00 - loss: 0.3979 - accuracy: 0.871 - ETA: 1:17:48 - loss: 0.3976 - accuracy: 0.870 - ETA: 1:17:37 - loss: 0.3976 - accuracy: 0.870 - ETA: 1:17:25 - loss: 0.3980 - accuracy: 0.870 - ETA: 1:17:14 - loss: 0.3963 - accuracy: 0.871 - ETA: 1:17:03 - loss: 0.3990 - accuracy: 0.870 - ETA: 1:16:52 - loss: 0.3990 - accuracy: 0.870 - ETA: 1:16:41 - loss: 0.3994 - accuracy: 0.870 - ETA: 1:16:29 - loss: 0.3987 - accuracy: 0.871 - ETA: 1:16:18 - loss: 0.3995 - accuracy: 0.870 - ETA: 1:16:07 - loss: 0.3982 - accuracy: 0.870 - ETA: 1:15:55 - loss: 0.3984 - accuracy: 0.870 - ETA: 1:15:44 - loss: 0.3976 - accuracy: 0.871 - ETA: 1:15:33 - loss: 0.3967 - accuracy: 0.871 - ETA: 1:15:22 - loss: 0.3971 - accuracy: 0.871 - ETA: 1:15:11 - loss: 0.3969 - accuracy: 0.871 - ETA: 1:14:59 - loss: 0.3954 - accuracy: 0.872 - ETA: 1:14:48 - loss: 0.3978 - accuracy: 0.871 - ETA: 1:14:36 - loss: 0.3976 - accuracy: 0.871 - ETA: 1:14:25 - loss: 0.3974 - accuracy: 0.871 - ETA: 1:14:14 - loss: 0.3965 - accuracy: 0.871 - ETA: 1:14:03 - loss: 0.3963 - accuracy: 0.871 - ETA: 1:13:51 - loss: 0.3976 - accuracy: 0.870 - ETA: 1:13:40 - loss: 0.3966 - accuracy: 0.871 - ETA: 1:13:29 - loss: 0.3954 - accuracy: 0.871 - ETA: 1:13:17 - loss: 0.3952 - accuracy: 0.871 - ETA: 1:13:06 - loss: 0.3944 - accuracy: 0.872 - ETA: 1:12:54 - loss: 0.3952 - accuracy: 0.871 - ETA: 1:12:43 - loss: 0.3946 - accuracy: 0.872 - ETA: 1:12:32 - loss: 0.3937 - accuracy: 0.872 - ETA: 1:12:20 - loss: 0.3945 - accuracy: 0.872 - ETA: 1:12:09 - loss: 0.3942 - accuracy: 0.872 - ETA: 1:11:57 - loss: 0.3949 - accuracy: 0.872 - ETA: 1:11:46 - loss: 0.3954 - accuracy: 0.871 - ETA: 1:11:35 - loss: 0.3949 - accuracy: 0.872 - ETA: 1:11:23 - loss: 0.3946 - accuracy: 0.872 - ETA: 1:11:11 - loss: 0.3943 - accuracy: 0.872 - ETA: 1:11:00 - loss: 0.3939 - accuracy: 0.872 - ETA: 1:10:48 - loss: 0.3936 - accuracy: 0.872 - ETA: 1:10:37 - loss: 0.3944 - accuracy: 0.872 - ETA: 1:10:26 - loss: 0.3939 - accuracy: 0.872 - ETA: 1:10:14 - loss: 0.3931 - accuracy: 0.872 - ETA: 1:10:03 - loss: 0.3932 - accuracy: 0.872 - ETA: 1:09:51 - loss: 0.3938 - accuracy: 0.872 - ETA: 1:09:40 - loss: 0.3932 - accuracy: 0.872 - ETA: 1:09:28 - loss: 0.3930 - accuracy: 0.872 - ETA: 1:09:17 - loss: 0.3924 - accuracy: 0.872 - ETA: 1:09:05 - loss: 0.3931 - accuracy: 0.872 - ETA: 1:08:53 - loss: 0.3925 - accuracy: 0.872 - ETA: 1:08:41 - loss: 0.3923 - accuracy: 0.872 - ETA: 1:08:30 - loss: 0.3934 - accuracy: 0.872 - ETA: 1:08:19 - loss: 0.3931 - accuracy: 0.872 - ETA: 1:08:07 - loss: 0.3932 - accuracy: 0.872 - ETA: 1:07:56 - loss: 0.3933 - accuracy: 0.872 - ETA: 1:07:45 - loss: 0.3936 - accuracy: 0.872 - ETA: 1:07:33 - loss: 0.3934 - accuracy: 0.872 - ETA: 1:07:22 - loss: 0.3928 - accuracy: 0.872 - ETA: 1:07:11 - loss: 0.3935 - accuracy: 0.872 - ETA: 1:07:00 - loss: 0.3940 - accuracy: 0.871 - ETA: 1:06:49 - loss: 0.3953 - accuracy: 0.871 - ETA: 1:06:38 - loss: 0.3946 - accuracy: 0.871 - ETA: 1:06:27 - loss: 0.3946 - accuracy: 0.871 - ETA: 1:06:15 - loss: 0.3944 - accuracy: 0.871 - ETA: 1:06:04 - loss: 0.3939 - accuracy: 0.872 - ETA: 1:05:53 - loss: 0.3935 - accuracy: 0.872 - ETA: 1:05:41 - loss: 0.3941 - accuracy: 0.871 - ETA: 1:05:30 - loss: 0.3950 - accuracy: 0.871 - ETA: 1:05:19 - loss: 0.3946 - accuracy: 0.871 - ETA: 1:05:08 - loss: 0.3955 - accuracy: 0.871 - ETA: 1:04:56 - loss: 0.3957 - accuracy: 0.871 - ETA: 1:04:45 - loss: 0.3952 - accuracy: 0.871 - ETA: 1:04:34 - loss: 0.3966 - accuracy: 0.871 - ETA: 1:04:22 - loss: 0.3964 - accuracy: 0.871 - ETA: 1:04:11 - loss: 0.3962 - accuracy: 0.871 - ETA: 1:04:00 - loss: 0.3957 - accuracy: 0.871 - ETA: 1:03:49 - loss: 0.3961 - accuracy: 0.871 - ETA: 1:03:38 - loss: 0.3970 - accuracy: 0.8707347/512 [===================>..........] - ETA: 1:03:27 - loss: 0.3969 - accuracy: 0.870 - ETA: 1:03:15 - loss: 0.3967 - accuracy: 0.870 - ETA: 1:03:04 - loss: 0.3983 - accuracy: 0.870 - ETA: 1:02:53 - loss: 0.3980 - accuracy: 0.870 - ETA: 1:02:42 - loss: 0.3988 - accuracy: 0.870 - ETA: 1:02:31 - loss: 0.3995 - accuracy: 0.870 - ETA: 1:02:20 - loss: 0.4003 - accuracy: 0.869 - ETA: 1:02:08 - loss: 0.3998 - accuracy: 0.869 - ETA: 1:01:57 - loss: 0.3998 - accuracy: 0.869 - ETA: 1:01:46 - loss: 0.3995 - accuracy: 0.870 - ETA: 1:01:35 - loss: 0.3997 - accuracy: 0.870 - ETA: 1:01:24 - loss: 0.3998 - accuracy: 0.870 - ETA: 1:01:13 - loss: 0.4000 - accuracy: 0.869 - ETA: 1:01:02 - loss: 0.3996 - accuracy: 0.870 - ETA: 1:00:51 - loss: 0.3991 - accuracy: 0.870 - ETA: 1:00:40 - loss: 0.3986 - accuracy: 0.870 - ETA: 1:00:28 - loss: 0.3978 - accuracy: 0.870 - ETA: 1:00:17 - loss: 0.3995 - accuracy: 0.870 - ETA: 1:00:06 - loss: 0.3993 - accuracy: 0.870 - ETA: 59:55 - loss: 0.3983 - accuracy: 0.8706  - ETA: 59:44 - loss: 0.3979 - accuracy: 0.870 - ETA: 59:32 - loss: 0.3971 - accuracy: 0.870 - ETA: 59:21 - loss: 0.3983 - accuracy: 0.870 - ETA: 59:10 - loss: 0.3981 - accuracy: 0.870 - ETA: 58:59 - loss: 0.3983 - accuracy: 0.870 - ETA: 58:47 - loss: 0.3982 - accuracy: 0.870 - ETA: 58:36 - loss: 0.3997 - accuracy: 0.870 - ETA: 58:25 - loss: 0.4007 - accuracy: 0.869 - ETA: 58:14 - loss: 0.4004 - accuracy: 0.869 - ETA: 58:02 - loss: 0.4003 - accuracy: 0.869 - ETA: 57:51 - loss: 0.4014 - accuracy: 0.869 - ETA: 57:40 - loss: 0.4011 - accuracy: 0.869 - ETA: 57:29 - loss: 0.4007 - accuracy: 0.869 - ETA: 57:18 - loss: 0.4003 - accuracy: 0.869 - ETA: 57:07 - loss: 0.4005 - accuracy: 0.869 - ETA: 56:55 - loss: 0.4004 - accuracy: 0.869 - ETA: 56:44 - loss: 0.3999 - accuracy: 0.869 - ETA: 56:33 - loss: 0.4003 - accuracy: 0.869 - ETA: 56:21 - loss: 0.3999 - accuracy: 0.869 - ETA: 56:10 - loss: 0.3994 - accuracy: 0.870 - ETA: 55:59 - loss: 0.3991 - accuracy: 0.870 - ETA: 55:48 - loss: 0.3990 - accuracy: 0.870 - ETA: 55:37 - loss: 0.3991 - accuracy: 0.870 - ETA: 55:25 - loss: 0.3989 - accuracy: 0.870 - ETA: 55:14 - loss: 0.3984 - accuracy: 0.870 - ETA: 55:04 - loss: 0.3984 - accuracy: 0.870 - ETA: 54:54 - loss: 0.3980 - accuracy: 0.870 - ETA: 54:44 - loss: 0.3981 - accuracy: 0.870 - ETA: 54:34 - loss: 0.3981 - accuracy: 0.870 - ETA: 54:24 - loss: 0.3978 - accuracy: 0.870 - ETA: 54:14 - loss: 0.3975 - accuracy: 0.870 - ETA: 54:04 - loss: 0.3970 - accuracy: 0.870 - ETA: 53:54 - loss: 0.3979 - accuracy: 0.870 - ETA: 53:44 - loss: 0.3977 - accuracy: 0.870 - ETA: 53:34 - loss: 0.3972 - accuracy: 0.870 - ETA: 53:24 - loss: 0.3980 - accuracy: 0.870 - ETA: 53:14 - loss: 0.3978 - accuracy: 0.870 - ETA: 53:04 - loss: 0.3982 - accuracy: 0.870 - ETA: 52:54 - loss: 0.3979 - accuracy: 0.870 - ETA: 52:44 - loss: 0.3971 - accuracy: 0.870 - ETA: 52:34 - loss: 0.3968 - accuracy: 0.870 - ETA: 52:24 - loss: 0.3970 - accuracy: 0.870 - ETA: 52:14 - loss: 0.3965 - accuracy: 0.871 - ETA: 52:04 - loss: 0.3957 - accuracy: 0.871 - ETA: 51:53 - loss: 0.3957 - accuracy: 0.871 - ETA: 51:43 - loss: 0.3954 - accuracy: 0.871 - ETA: 51:33 - loss: 0.3951 - accuracy: 0.871 - ETA: 51:23 - loss: 0.3958 - accuracy: 0.871 - ETA: 51:12 - loss: 0.3955 - accuracy: 0.871 - ETA: 51:02 - loss: 0.3953 - accuracy: 0.871 - ETA: 50:51 - loss: 0.3956 - accuracy: 0.871 - ETA: 50:41 - loss: 0.3955 - accuracy: 0.871 - ETA: 50:30 - loss: 0.3949 - accuracy: 0.871 - ETA: 50:19 - loss: 0.3949 - accuracy: 0.871 - ETA: 50:09 - loss: 0.3949 - accuracy: 0.871 - ETA: 49:58 - loss: 0.3949 - accuracy: 0.871 - ETA: 49:48 - loss: 0.3944 - accuracy: 0.871 - ETA: 49:37 - loss: 0.3941 - accuracy: 0.871 - ETA: 49:27 - loss: 0.3953 - accuracy: 0.871 - ETA: 49:16 - loss: 0.3954 - accuracy: 0.871 - ETA: 49:06 - loss: 0.3952 - accuracy: 0.871 - ETA: 48:55 - loss: 0.3948 - accuracy: 0.871 - ETA: 48:44 - loss: 0.3946 - accuracy: 0.871 - ETA: 48:34 - loss: 0.3940 - accuracy: 0.871 - ETA: 48:23 - loss: 0.3937 - accuracy: 0.871 - ETA: 48:12 - loss: 0.3935 - accuracy: 0.871 - ETA: 48:01 - loss: 0.3940 - accuracy: 0.871 - ETA: 47:51 - loss: 0.3939 - accuracy: 0.871 - ETA: 47:40 - loss: 0.3937 - accuracy: 0.871 - ETA: 47:29 - loss: 0.3946 - accuracy: 0.871 - ETA: 47:19 - loss: 0.3947 - accuracy: 0.871 - ETA: 47:08 - loss: 0.3946 - accuracy: 0.871 - ETA: 46:57 - loss: 0.3950 - accuracy: 0.871 - ETA: 46:47 - loss: 0.3947 - accuracy: 0.871 - ETA: 46:36 - loss: 0.3954 - accuracy: 0.870 - ETA: 46:25 - loss: 0.3955 - accuracy: 0.870 - ETA: 46:15 - loss: 0.3953 - accuracy: 0.871 - ETA: 46:04 - loss: 0.3958 - accuracy: 0.870 - ETA: 45:53 - loss: 0.3973 - accuracy: 0.870 - ETA: 45:42 - loss: 0.3969 - accuracy: 0.870 - ETA: 45:31 - loss: 0.3966 - accuracy: 0.870 - ETA: 45:20 - loss: 0.3965 - accuracy: 0.870 - ETA: 45:10 - loss: 0.3963 - accuracy: 0.870 - ETA: 44:59 - loss: 0.3963 - accuracy: 0.871 - ETA: 44:49 - loss: 0.3960 - accuracy: 0.871 - ETA: 44:38 - loss: 0.3966 - accuracy: 0.870 - ETA: 44:27 - loss: 0.3963 - accuracy: 0.871 - ETA: 44:16 - loss: 0.3964 - accuracy: 0.871 - ETA: 44:06 - loss: 0.3963 - accuracy: 0.871 - ETA: 43:55 - loss: 0.3964 - accuracy: 0.870 - ETA: 43:44 - loss: 0.3957 - accuracy: 0.871 - ETA: 43:33 - loss: 0.3955 - accuracy: 0.871 - ETA: 43:22 - loss: 0.3956 - accuracy: 0.871 - ETA: 43:11 - loss: 0.3952 - accuracy: 0.871 - ETA: 43:01 - loss: 0.3960 - accuracy: 0.871 - ETA: 42:50 - loss: 0.3959 - accuracy: 0.871 - ETA: 42:39 - loss: 0.3957 - accuracy: 0.871 - ETA: 42:28 - loss: 0.3956 - accuracy: 0.871 - ETA: 42:17 - loss: 0.3962 - accuracy: 0.871 - ETA: 42:06 - loss: 0.3966 - accuracy: 0.870 - ETA: 41:55 - loss: 0.3964 - accuracy: 0.871 - ETA: 41:44 - loss: 0.3964 - accuracy: 0.871 - ETA: 41:33 - loss: 0.3965 - accuracy: 0.870 - ETA: 41:22 - loss: 0.3960 - accuracy: 0.871 - ETA: 41:11 - loss: 0.3967 - accuracy: 0.870 - ETA: 41:01 - loss: 0.3968 - accuracy: 0.870 - ETA: 40:50 - loss: 0.3965 - accuracy: 0.870 - ETA: 40:39 - loss: 0.3961 - accuracy: 0.871 - ETA: 40:28 - loss: 0.3964 - accuracy: 0.870 - ETA: 40:17 - loss: 0.3962 - accuracy: 0.870 - ETA: 40:06 - loss: 0.3958 - accuracy: 0.871 - ETA: 39:55 - loss: 0.3957 - accuracy: 0.871 - ETA: 39:44 - loss: 0.3956 - accuracy: 0.871 - ETA: 39:33 - loss: 0.3955 - accuracy: 0.871 - ETA: 39:22 - loss: 0.3954 - accuracy: 0.871 - ETA: 39:11 - loss: 0.3949 - accuracy: 0.871 - ETA: 39:00 - loss: 0.3948 - accuracy: 0.871 - ETA: 38:49 - loss: 0.3949 - accuracy: 0.871 - ETA: 38:38 - loss: 0.3953 - accuracy: 0.871 - ETA: 38:27 - loss: 0.3953 - accuracy: 0.871 - ETA: 38:16 - loss: 0.3954 - accuracy: 0.871 - ETA: 38:04 - loss: 0.3952 - accuracy: 0.871 - ETA: 37:53 - loss: 0.3951 - accuracy: 0.871 - ETA: 37:42 - loss: 0.3947 - accuracy: 0.871 - ETA: 37:31 - loss: 0.3945 - accuracy: 0.871 - ETA: 37:20 - loss: 0.3947 - accuracy: 0.871 - ETA: 37:09 - loss: 0.3946 - accuracy: 0.871 - ETA: 36:58 - loss: 0.3944 - accuracy: 0.871 - ETA: 36:47 - loss: 0.3942 - accuracy: 0.871 - ETA: 36:36 - loss: 0.3944 - accuracy: 0.871 - ETA: 36:24 - loss: 0.3946 - accuracy: 0.871 - ETA: 36:13 - loss: 0.3945 - accuracy: 0.871 - ETA: 36:02 - loss: 0.3944 - accuracy: 0.871 - ETA: 35:51 - loss: 0.3940 - accuracy: 0.871 - ETA: 35:40 - loss: 0.3943 - accuracy: 0.871 - ETA: 35:28 - loss: 0.3940 - accuracy: 0.871 - ETA: 35:17 - loss: 0.3938 - accuracy: 0.871 - ETA: 35:06 - loss: 0.3943 - accuracy: 0.871 - ETA: 34:55 - loss: 0.3947 - accuracy: 0.871 - ETA: 34:44 - loss: 0.3943 - accuracy: 0.871 - ETA: 34:32 - loss: 0.3950 - accuracy: 0.871 - ETA: 34:21 - loss: 0.3949 - accuracy: 0.871 - ETA: 34:10 - loss: 0.3945 - accuracy: 0.871 - ETA: 33:59 - loss: 0.3944 - accuracy: 0.871 - ETA: 33:48 - loss: 0.3947 - accuracy: 0.871 - ETA: 33:36 - loss: 0.3946 - accuracy: 0.871 - ETA: 33:25 - loss: 0.3948 - accuracy: 0.871 - ETA: 33:14 - loss: 0.3948 - accuracy: 0.871 - ETA: 33:03 - loss: 0.3948 - accuracy: 0.871 - ETA: 32:51 - loss: 0.3947 - accuracy: 0.871 - ETA: 32:40 - loss: 0.3945 - accuracy: 0.871 - ETA: 32:29 - loss: 0.3943 - accuracy: 0.871 - ETA: 32:17 - loss: 0.3945 - accuracy: 0.871 - ETA: 32:06 - loss: 0.3945 - accuracy: 0.871 - ETA: 31:55 - loss: 0.3950 - accuracy: 0.871 - ETA: 31:44 - loss: 0.3951 - accuracy: 0.871 - ETA: 31:32 - loss: 0.3948 - accuracy: 0.8712"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351/512 [===================>..........] - ETA: 31:21 - loss: 0.3949 - accuracy: 0.871 - ETA: 31:10 - loss: 0.3954 - accuracy: 0.871 - ETA: 30:59 - loss: 0.3952 - accuracy: 0.871 - ETA: 30:48 - loss: 0.3949 - accuracy: 0.8712"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-206365b51eb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m new_model.train(\n\u001b[0;32m      2\u001b[0m     \u001b[0mtrain_images\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;34m\"dataset1/images_prepped_train/\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain_annotations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"dataset1/annotations_prepped_train/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m )\n",
      "\u001b[1;32mc:\\users\\vash\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras_segmentation-0.3.0-py3.7.egg\\keras_segmentation\\train.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_images, train_annotations, input_height, input_width, n_classes, verify_dataset, checkpoints_path, epochs, batch_size, validate, val_images, val_annotations, val_batch_size, auto_resume_checkpoint, load_weights, steps_per_epoch, optimizer_name)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Starting Epoch \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcheckpoints_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoints_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vash\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vash\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vash\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m                                             reset_metrics=False)\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vash\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\vash\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3476\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32mc:\\users\\vash\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "new_model.train(\n",
    "    train_images =  \"dataset1/images_prepped_train/\",\n",
    "    train_annotations = \"dataset1/annotations_prepped_train/\", epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OS\n",
      " Volume Serial Number is 7A94-1164\n",
      "\n",
      " Directory of C:\\apps\\segmentation\n",
      "\n",
      "11/01/2019  05:45 PM    <DIR>          .\n",
      "11/01/2019  05:45 PM    <DIR>          ..\n",
      "11/01/2019  04:36 PM    <DIR>          .ipynb_checkpoints\n",
      "09/22/2017  07:20 PM    <DIR>          dataset1\n",
      "11/01/2019  05:45 PM           230,956 pspnet_50_train.ipynb\n",
      "11/01/2019  03:15 PM    <DIR>          tmp\n",
      "11/01/2019  03:16 PM            21,834 Untitled.ipynb\n",
      "11/01/2019  04:34 PM                72 Untitled2.ipynb\n",
      "               3 File(s)        252,862 bytes\n",
      "               5 Dir(s)  159,942,717,440 bytes free\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.save('pspnet_50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OS\n",
      " Volume Serial Number is 7A94-1164\n",
      "\n",
      " Directory of C:\\apps\\segmentation\n",
      "\n",
      "11/01/2019  05:48 PM    <DIR>          .\n",
      "11/01/2019  05:48 PM    <DIR>          ..\n",
      "11/01/2019  04:36 PM    <DIR>          .ipynb_checkpoints\n",
      "09/22/2017  07:20 PM    <DIR>          dataset1\n",
      "11/01/2019  05:48 PM       187,960,272 pspnet_50.h5\n",
      "11/01/2019  05:47 PM           242,468 pspnet_50_train.ipynb\n",
      "11/01/2019  03:15 PM    <DIR>          tmp\n",
      "11/01/2019  03:16 PM            21,834 Untitled.ipynb\n",
      "11/01/2019  04:34 PM                72 Untitled2.ipynb\n",
      "               4 File(s)    188,224,646 bytes\n",
      "               5 Dir(s)  159,754,686,464 bytes free\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = new_model.predict_segmentation(\n",
    "    inp=\"dataset1/images_prepped_test/0016E5_07965.png\",\n",
    "    out_fname=\"out.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26f89128c08>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "101it [07:46,  4.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'frequency_weighted_IU': 0.7874489358062977, 'mean_IU': 0.12051682462747976, 'class_wise_IU': array([0.89717845, 0.8422267 , 0.08687355, 0.89951006, 0.66602251,\n",
      "       0.83728578, 0.29996345, 0.05891924, 0.78528028, 0.2319913 ,\n",
      "       0.3365503 , 0.20455644, 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
      "       0.        ])}\n"
     ]
    }
   ],
   "source": [
    "# evaluating the model\n",
    "print(new_model.evaluate_segmentation( inp_images_dir=\"dataset1/images_prepped_test/\" , annotations_dir=\"dataset1/annotations_prepped_test/\" ))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vgg_unet \n",
    " [01:37,  1.14it/s]\n",
    "{'frequency_weighted_IU': 0.8113695628044716, 'mean_IU': 0.11645628869542403, \n",
    " 'class_wise_IU': array([0.93353119, 0.84831478, 0.0787614 , 0.94478497, 0.79711673,\n",
    "       0.82216835, 0.16882607, 0.29642238, 0.6474678 , 0.13879422,\n",
    "       0.06406401, 0.19901882, 0.        , 0.        , 0.        ,\n",
    "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
    "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
    "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
    "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
    "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
    "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
    "       0.        ])}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
